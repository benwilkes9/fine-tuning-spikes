{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Deploy and Evaluate Fine-Tuned Gemma-3-4B for Financial Sentiment\n",
        "---\n",
        "This notebook deploys the fine-tuned **Gemma-3-4B-IT** model from notebook 2 and evaluates it on the held-out test set.\n",
        "\n",
        "**Prerequisites:**\n",
        "- Run notebook 1 (`01_data_analysis.ipynb`) to generate training data\n",
        "- Run notebook 2 (`02_finetune_training.ipynb`) to fine-tune the model\n",
        "- Training job must be completed successfully\n",
        "\n",
        "**What this notebook does:**\n",
        "1. Extract model artifacts from S3\n",
        "2. Deploy model to SageMaker endpoint using DJL LMI (vLLM)\n",
        "3. Evaluate on 200 test samples from `test_data.jsonl`\n",
        "4. Calculate accuracy, precision, recall, F1-score\n",
        "5. Cleanup resources\n",
        "\n",
        "---\n",
        "\n",
        "**Model:** [google/gemma-3-4b-it](https://huggingface.co/google/gemma-3-4b-it)  \n",
        "**Instance:** ml.g5.2xlarge (1x A10G, 24GB VRAM)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Setup and Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import boto3\n",
        "import sagemaker\n",
        "from sagemaker.session import Session\n",
        "import json\n",
        "import time\n",
        "import os\n",
        "import pandas as pd\n",
        "from getpass import getpass\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Configuration\n",
        "region = \"eu-west-2\"\n",
        "role = \"arn:aws:iam::889772146711:role/SageMakerExecutionRole\"\n",
        "s3_bucket = \"sagemaker-eu-west-2-889772146711\"\n",
        "training_job_name = \"google--gemma-3-4b-it-sentiment-finetune-20260107185405\"\n",
        "\n",
        "# S3 paths (matches notebook 02 output structure)\n",
        "base_job_name = \"google--gemma-3-4b-it-sentiment-finetune\"\n",
        "s3_model_uri = f\"s3://{s3_bucket}/{base_job_name}/{training_job_name}/output/model.tar.gz\"\n",
        "s3_uncompressed_prefix = f\"{base_job_name}/{training_job_name}/output/uncompressed/\"\n",
        "s3_model_prefix = f\"s3://{s3_bucket}/{s3_uncompressed_prefix}\"\n",
        "\n",
        "# Create clients\n",
        "sess = Session(boto3.Session(region_name=region))\n",
        "sm_client = boto3.client(\"sagemaker\", region_name=region)\n",
        "s3_client = boto3.client(\"s3\", region_name=region)\n",
        "sagemaker_runtime = boto3.client(\"sagemaker-runtime\", region_name=region)\n",
        "\n",
        "print(f\"Training job: {training_job_name}\")\n",
        "print(f\"Model artifacts: {s3_model_uri}\")\n",
        "print(f\"Uncompressed path: {s3_model_prefix}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# HuggingFace token (required for Gemma gated model)\n",
        "hf_token = getpass(\"Enter HuggingFace token: \")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Prepare Model Artifacts\n",
        "\n",
        "Extract `model.tar.gz` to S3 (streaming, no local download) and add missing `preprocessor_config.json` for vLLM compatibility."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%%time\n",
        "import tarfile\n",
        "from smart_open import open as smart_open\n",
        "from huggingface_hub import hf_hub_download\n",
        "\n",
        "# Check if already extracted\n",
        "response = s3_client.list_objects_v2(\n",
        "    Bucket=s3_bucket, \n",
        "    Prefix=s3_uncompressed_prefix + \"google/gemma-3-4b-it/\",\n",
        "    MaxKeys=5\n",
        ")\n",
        "already_extracted = response.get(\"KeyCount\", 0) > 0\n",
        "\n",
        "# Extract if needed\n",
        "if not already_extracted:\n",
        "    print(\"Extracting model.tar.gz to S3 (streaming - no local download)...\")\n",
        "    print(\"This may take 5-10 minutes.\")\n",
        "    \n",
        "    file_count = 0\n",
        "    total_bytes = 0\n",
        "    \n",
        "    with smart_open(s3_model_uri, \"rb\") as f:\n",
        "        with tarfile.open(fileobj=f, mode=\"r:*\") as tar:\n",
        "            for member in tar:\n",
        "                if member.isfile():\n",
        "                    file_obj = tar.extractfile(member)\n",
        "                    if file_obj:\n",
        "                        content = file_obj.read()\n",
        "                        s3_client.put_object(\n",
        "                            Bucket=s3_bucket,\n",
        "                            Key=s3_uncompressed_prefix + member.name,\n",
        "                            Body=content\n",
        "                        )\n",
        "                        file_count += 1\n",
        "                        total_bytes += len(content)\n",
        "                        if file_count % 10 == 0:\n",
        "                            print(f\"  Extracted {file_count} files ({total_bytes / 1e9:.2f} GB)...\")\n",
        "    \n",
        "    print(f\"Extraction complete: {file_count} files, {total_bytes / 1e9:.2f} GB\")\n",
        "else:\n",
        "    print(\"Model already extracted. Skipping.\")\n",
        "\n",
        "# Add missing preprocessor_config.json (required for vLLM with Gemma 3 VLM)\n",
        "print(\"\\nAdding preprocessor_config.json from HuggingFace...\")\n",
        "config_path = hf_hub_download(repo_id=\"google/gemma-3-4b-it\", filename=\"preprocessor_config.json\", token=hf_token)\n",
        "with open(config_path, \"rb\") as f:\n",
        "    s3_client.put_object(\n",
        "        Bucket=s3_bucket,\n",
        "        Key=s3_uncompressed_prefix + \"google/gemma-3-4b-it/preprocessor_config.json\",\n",
        "        Body=f.read()\n",
        "    )\n",
        "print(f\"Model artifacts ready at: {s3_model_prefix}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Create SageMaker Resources\n",
        "\n",
        "Deploy using DJL LMI container with vLLM backend."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Resource names\n",
        "model_name = sagemaker.utils.name_from_base(\"gemma-3-4b-sentiment\")\n",
        "endpoint_config_name = f\"epc-{model_name}\"\n",
        "endpoint_name = f\"ep-{model_name}\"\n",
        "inference_component_name = f\"ic-{model_name}\"\n",
        "variant_name = \"AllTraffic\"\n",
        "instance_type = \"ml.g5.2xlarge\"  # 1x A10G (24GB)\n",
        "num_gpu = 1\n",
        "\n",
        "print(f\"Model: {model_name}\")\n",
        "print(f\"Endpoint: {endpoint_name}\")\n",
        "print(f\"Instance: {instance_type}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create Model\n",
        "inference_image = f\"763104351884.dkr.ecr.{region}.amazonaws.com/djl-inference:0.34.0-lmi16.0.0-cu128\"\n",
        "\n",
        "sm_client.create_model(\n",
        "    ModelName=model_name,\n",
        "    ExecutionRoleArn=role,\n",
        "    PrimaryContainer={\n",
        "        \"Image\": inference_image,\n",
        "        \"ModelDataSource\": {\n",
        "            \"S3DataSource\": {\n",
        "                \"S3Uri\": s3_model_prefix,\n",
        "                \"S3DataType\": \"S3Prefix\",\n",
        "                \"CompressionType\": \"None\",\n",
        "            }\n",
        "        },\n",
        "        \"Environment\": {\n",
        "            \"HF_TOKEN\": hf_token,\n",
        "            \"SAGEMAKER_MODEL_SERVER_WORKERS\": \"1\",\n",
        "            \"MESSAGES_API_ENABLED\": \"true\",\n",
        "            \"OPTION_MAX_ROLLING_BATCH_SIZE\": \"8\",\n",
        "            \"OPTION_MODEL_LOADING_TIMEOUT\": \"1500\",\n",
        "            \"SERVING_FAIL_FAST\": \"true\",\n",
        "            \"OPTION_ROLLING_BATCH\": \"disable\",\n",
        "            \"OPTION_ASYNC_MODE\": \"true\",\n",
        "            \"OPTION_ENTRYPOINT\": \"djl_python.lmi_vllm.vllm_async_service\",\n",
        "            \"OPTION_ENABLE_STREAMING\": \"true\",\n",
        "            \"MAX_TOTAL_TOKENS\": \"4096\",\n",
        "        },\n",
        "    },\n",
        ")\n",
        "print(f\"Created model: {model_name}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create Endpoint Configuration\n",
        "sm_client.create_endpoint_config(\n",
        "    EndpointConfigName=endpoint_config_name,\n",
        "    ExecutionRoleArn=role,\n",
        "    ProductionVariants=[{\n",
        "        \"VariantName\": variant_name,\n",
        "        \"InstanceType\": instance_type,\n",
        "        \"InitialInstanceCount\": 1,\n",
        "        \"ModelDataDownloadTimeoutInSeconds\": 3600,\n",
        "        \"ContainerStartupHealthCheckTimeoutInSeconds\": 3600,\n",
        "        \"ManagedInstanceScaling\": {\"Status\": \"ENABLED\", \"MinInstanceCount\": 1, \"MaxInstanceCount\": 1},\n",
        "        \"RoutingConfig\": {\"RoutingStrategy\": \"LEAST_OUTSTANDING_REQUESTS\"},\n",
        "    }],\n",
        ")\n",
        "print(f\"Created endpoint config: {endpoint_config_name}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Deploy Endpoint\n",
        "sm_client.create_endpoint(EndpointName=endpoint_name, EndpointConfigName=endpoint_config_name)\n",
        "print(f\"Creating endpoint: {endpoint_name}\")\n",
        "print(\"Waiting for endpoint (this may take several minutes)...\")\n",
        "\n",
        "sess.wait_for_endpoint(endpoint_name)\n",
        "print(f\"Endpoint {endpoint_name} is InService!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create Inference Component\n",
        "sm_client.create_inference_component(\n",
        "    InferenceComponentName=inference_component_name,\n",
        "    EndpointName=endpoint_name,\n",
        "    VariantName=variant_name,\n",
        "    Specification={\n",
        "        \"ModelName\": model_name,\n",
        "        \"ComputeResourceRequirements\": {\n",
        "            \"NumberOfAcceleratorDevicesRequired\": num_gpu,\n",
        "            \"NumberOfCpuCoresRequired\": 1,\n",
        "            \"MinMemoryRequiredInMb\": 1024,\n",
        "        },\n",
        "    },\n",
        "    RuntimeConfig={\"CopyCount\": 1},\n",
        ")\n",
        "print(f\"Creating inference component: {inference_component_name}\")\n",
        "print(\"Waiting for inference component (this may take ~10 minutes)...\")\n",
        "\n",
        "start_time = time.time()\n",
        "while True:\n",
        "    desc = sm_client.describe_inference_component(InferenceComponentName=inference_component_name)\n",
        "    status = desc[\"InferenceComponentStatus\"]\n",
        "    if status in [\"InService\", \"Failed\"]:\n",
        "        break\n",
        "    print(f\"  Status: {status}\")\n",
        "    time.sleep(30)\n",
        "\n",
        "if status == \"Failed\":\n",
        "    raise Exception(f\"Inference component failed: {desc.get('FailureReason', 'Unknown')}\")\n",
        "\n",
        "print(f\"\\nInference component ready! ({(time.time() - start_time)/60:.1f} minutes)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Test Inference\n",
        "\n",
        "Quick sanity check before running full evaluation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def invoke_model(messages, max_tokens=32, temperature=0.1):\n",
        "    \"\"\"Invoke the deployed model with messages.\"\"\"\n",
        "    payload = {\n",
        "        \"messages\": messages,\n",
        "        \"temperature\": temperature,\n",
        "        \"top_p\": 0.9,\n",
        "        \"max_tokens\": max_tokens,\n",
        "    }\n",
        "    \n",
        "    response = sagemaker_runtime.invoke_endpoint(\n",
        "        EndpointName=endpoint_name,\n",
        "        InferenceComponentName=inference_component_name,\n",
        "        ContentType=\"application/json\",\n",
        "        Body=json.dumps(payload)\n",
        "    )\n",
        "    \n",
        "    result = json.loads(response[\"Body\"].read().decode(\"utf-8\"))\n",
        "    return result[\"choices\"][0][\"message\"][\"content\"].strip().lower()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Quick test\n",
        "test_messages = [\n",
        "    {\"role\": \"system\", \"content\": \"You are a financial sentiment analysis expert. Your task is to analyze the sentiment expressed in the given financial text.Only reply with positive, neutral, or negative.\"},\n",
        "    {\"role\": \"user\", \"content\": \"Apple reported record quarterly revenue of $123.9 billion, up 11% year over year.\"}\n",
        "]\n",
        "\n",
        "response = invoke_model(test_messages)\n",
        "print(f\"Test response: {response}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Evaluate on Test Set\n",
        "\n",
        "Run inference on all 200 test samples and calculate metrics."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load test data\n",
        "test_data_path = os.path.join(os.getcwd(), \"tmp_cache_local_dataset\", \"test_data.jsonl\")\n",
        "\n",
        "test_samples = []\n",
        "with open(test_data_path, \"r\") as f:\n",
        "    for line in f:\n",
        "        test_samples.append(json.loads(line))\n",
        "\n",
        "print(f\"Loaded {len(test_samples)} test samples\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Run inference on all test samples\n",
        "predictions = []\n",
        "ground_truth = []\n",
        "errors = []\n",
        "\n",
        "print(\"Running inference on test set...\")\n",
        "for i, sample in enumerate(tqdm(test_samples)):\n",
        "    messages = sample[\"messages\"]\n",
        "    \n",
        "    # Extract ground truth (assistant message)\n",
        "    expected = messages[-1][\"content\"].strip().lower()\n",
        "    ground_truth.append(expected)\n",
        "    \n",
        "    # Get model prediction (only system + user messages)\n",
        "    input_messages = [m for m in messages if m[\"role\"] != \"assistant\"]\n",
        "    \n",
        "    try:\n",
        "        predicted = invoke_model(input_messages)\n",
        "        # Normalize prediction to expected labels\n",
        "        predicted = predicted.split()[0] if predicted else \"unknown\"  # Take first word\n",
        "        predictions.append(predicted)\n",
        "    except Exception as e:\n",
        "        errors.append((i, str(e)))\n",
        "        predictions.append(\"error\")\n",
        "    \n",
        "    # Rate limiting - small delay between requests\n",
        "    time.sleep(0.1)\n",
        "\n",
        "print(f\"\\nCompleted: {len(predictions)} predictions, {len(errors)} errors\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "\n",
        "# Filter out errors for metric calculation\n",
        "valid_indices = [i for i, p in enumerate(predictions) if p != \"error\"]\n",
        "valid_predictions = [predictions[i] for i in valid_indices]\n",
        "valid_ground_truth = [ground_truth[i] for i in valid_indices]\n",
        "\n",
        "print(f\"Valid samples for evaluation: {len(valid_predictions)}/{len(predictions)}\")\n",
        "print()\n",
        "\n",
        "# Calculate metrics\n",
        "accuracy = accuracy_score(valid_ground_truth, valid_predictions)\n",
        "print(f\"Overall Accuracy: {accuracy:.4f} ({accuracy*100:.2f}%)\")\n",
        "print()\n",
        "\n",
        "# Detailed classification report\n",
        "print(\"Classification Report:\")\n",
        "print(\"=\" * 60)\n",
        "print(classification_report(valid_ground_truth, valid_predictions, zero_division=0))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Confusion Matrix\n",
        "labels = sorted(list(set(valid_ground_truth + valid_predictions)))\n",
        "cm = confusion_matrix(valid_ground_truth, valid_predictions, labels=labels)\n",
        "\n",
        "print(\"Confusion Matrix:\")\n",
        "print(\"=\" * 60)\n",
        "cm_df = pd.DataFrame(cm, index=labels, columns=labels)\n",
        "cm_df.index.name = \"Actual\"\n",
        "cm_df.columns.name = \"Predicted\"\n",
        "print(cm_df)\n",
        "print()\n",
        "print(\"Rows = Actual labels, Columns = Predicted labels\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Sample predictions (first 10)\n",
        "print(\"Sample Predictions (first 10):\")\n",
        "print(\"=\" * 80)\n",
        "for i in range(min(10, len(test_samples))):\n",
        "    user_msg = test_samples[i][\"messages\"][1][\"content\"][:80]  # Truncate long text\n",
        "    expected = ground_truth[i]\n",
        "    predicted = predictions[i]\n",
        "    match = \"✓\" if expected == predicted else \"✗\"\n",
        "    print(f\"{match} Expected: {expected:10} | Predicted: {predicted:10} | Text: {user_msg}...\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Show misclassified examples\n",
        "print(\"Misclassified Examples (first 10):\")\n",
        "print(\"=\" * 80)\n",
        "misclassified = [(i, ground_truth[i], predictions[i]) for i in range(len(predictions)) \n",
        "                 if ground_truth[i] != predictions[i] and predictions[i] != \"error\"]\n",
        "\n",
        "for idx, (i, expected, predicted) in enumerate(misclassified[:10]):\n",
        "    user_msg = test_samples[i][\"messages\"][1][\"content\"][:100]\n",
        "    print(f\"{idx+1}. Expected: {expected:10} | Predicted: {predicted:10}\")\n",
        "    print(f\"   Text: {user_msg}...\")\n",
        "    print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Save Results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save evaluation results\n",
        "results_df = pd.DataFrame({\n",
        "    \"text\": [s[\"messages\"][1][\"content\"] for s in test_samples],\n",
        "    \"expected\": ground_truth,\n",
        "    \"predicted\": predictions,\n",
        "    \"correct\": [g == p for g, p in zip(ground_truth, predictions)]\n",
        "})\n",
        "\n",
        "results_path = os.path.join(os.getcwd(), \"evaluation_results.csv\")\n",
        "results_df.to_csv(results_path, index=False)\n",
        "print(f\"Results saved to: {results_path}\")\n",
        "\n",
        "# Summary\n",
        "print(f\"\\nSummary:\")\n",
        "print(f\"  Total samples: {len(test_samples)}\")\n",
        "print(f\"  Correct: {results_df['correct'].sum()}\")\n",
        "print(f\"  Incorrect: {(~results_df['correct']).sum()}\")\n",
        "print(f\"  Accuracy: {results_df['correct'].mean()*100:.2f}%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Cleanup\n",
        "\n",
        "Delete all SageMaker resources to stop billing. **Only run this when you're done with the endpoint.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import botocore\n",
        "\n",
        "def delete_resource(delete_fn, name, rtype):\n",
        "    try:\n",
        "        delete_fn()\n",
        "        print(f\"✓ Deleted {rtype}: {name}\")\n",
        "    except botocore.exceptions.ClientError as e:\n",
        "        if e.response['Error']['Code'] in ['ValidationException', 'ResourceNotFound']:\n",
        "            print(f\"⊘ {rtype} not found: {name}\")\n",
        "        else:\n",
        "            raise"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Run this cell to perform cleanup\n",
        "delete_resource(lambda: sm_client.delete_inference_component(InferenceComponentName=inference_component_name), inference_component_name, \"Inference component\")\n",
        "print(\"Waiting 60s for inference component deletion...\")\n",
        "time.sleep(60)\n",
        "delete_resource(lambda: sm_client.delete_endpoint(EndpointName=endpoint_name), endpoint_name, \"Endpoint\")\n",
        "delete_resource(lambda: sm_client.delete_endpoint_config(EndpointConfigName=endpoint_config_name), endpoint_config_name, \"Endpoint config\")\n",
        "delete_resource(lambda: sm_client.delete_model(ModelName=model_name), model_name, \"Model\")\n",
        "print(\"\\n✓ Cleanup complete!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
