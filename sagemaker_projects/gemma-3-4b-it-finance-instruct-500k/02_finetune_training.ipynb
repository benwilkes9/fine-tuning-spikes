{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Fine-tune `google/gemma-3-4b-it` for Financial Sentiment Analysis\n",
        "---\n",
        "This notebook fine-tunes **Gemma-3-4B-IT** on a financial sentiment analysis dataset using Amazon SageMaker.\n",
        "\n",
        "**Prerequisites:**\n",
        "- Run notebook 1 (`01_data_analysis.ipynb`) first to generate `sentiment_training_data.csv`\n",
        "\n",
        "**What this notebook does:**\n",
        "1. Load the sentiment training data from notebook 1\n",
        "2. Split into 90% training / 10% test sets\n",
        "3. Convert to messages format (JSONL)\n",
        "4. Upload training data to S3\n",
        "5. Launch a SageMaker training job using QLoRA (PEFT)\n",
        "\n",
        "---\n",
        "\n",
        "**Model:** [google/gemma-3-4b-it](https://huggingface.co/google/gemma-3-4b-it)  \n",
        "**Training Method:** PEFT QLoRA (4-bit quantization with LoRA adapters)  \n",
        "**Instance:** ml.g5.2xlarge\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Setup and Dependencies\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import json\n",
        "import pandas as pd\n",
        "import boto3\n",
        "import sagemaker\n",
        "from datasets import Dataset\n",
        "from sklearn.model_selection import train_test_split\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "region = boto3.Session().region_name\n",
        "\n",
        "sess = sagemaker.Session(boto3.Session(region_name=region))\n",
        "\n",
        "sagemaker_session_bucket = None\n",
        "if sagemaker_session_bucket is None and sess is not None:\n",
        "    sagemaker_session_bucket = sess.default_bucket()\n",
        "\n",
        "try:\n",
        "    role = sagemaker.get_execution_role()\n",
        "except ValueError:\n",
        "    # Fallback to an explicit SageMaker execution role ARN if not using a SageMaker execution role\n",
        "    role = \"arn:aws:iam::889772146711:role/SageMakerExecutionRole\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(f\"sagemaker role arn: {role}\")\n",
        "print(f\"sagemaker bucket: {sess.default_bucket()}\")\n",
        "print(f\"sagemaker session region: {sess.boto_region_name}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Data Preparation\n",
        "\n",
        "Load the sentiment training data from notebook 1, split into train/test, and convert to messages format.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create output directory\n",
        "dataset_parent_path = os.path.join(os.getcwd(), \"tmp_cache_local_dataset\")\n",
        "os.makedirs(dataset_parent_path, exist_ok=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load sentiment training data from notebook 1\n",
        "csv_path = os.path.join(os.getcwd(), \"sentiment_training_data.csv\")\n",
        "df = pd.read_csv(csv_path)\n",
        "\n",
        "print(f\"Loaded {len(df)} samples from sentiment_training_data.csv\")\n",
        "print(f\"\\nColumns: {list(df.columns)}\")\n",
        "print(f\"\\nLabel distribution:\")\n",
        "print(df['assistant'].value_counts())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Split data: 90% training, 10% test\n",
        "# The training data will be further split internally by sft.py (90/10) for train/eval\n",
        "train_df, test_df = train_test_split(\n",
        "    df, \n",
        "    test_size=0.10, \n",
        "    random_state=42, \n",
        "    stratify=df['assistant']  # Maintain label distribution\n",
        ")\n",
        "\n",
        "print(f\"Training samples: {len(train_df)} (90%)\")\n",
        "print(f\"Test samples: {len(test_df)} (10%)\")\n",
        "print(f\"\\nTraining label distribution:\")\n",
        "print(train_df['assistant'].value_counts())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Convert to Messages Format\n",
        "\n",
        "The SFT training script expects data in the `messages` format:\n",
        "```json\n",
        "{\n",
        "  \"messages\": [\n",
        "    { \"role\": \"system\", \"content\": \"...\" },\n",
        "    { \"role\": \"user\", \"content\": \"...\" },\n",
        "    { \"role\": \"assistant\", \"content\": \"...\" }\n",
        "  ]\n",
        "}\n",
        "```\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def convert_to_messages(row):\n",
        "    \"\"\"Convert a row to messages format.\"\"\"\n",
        "    return {\n",
        "        \"messages\": [\n",
        "            {\"role\": \"system\", \"content\": row[\"system\"]},\n",
        "            {\"role\": \"user\", \"content\": row[\"user\"]},\n",
        "            {\"role\": \"assistant\", \"content\": row[\"assistant\"]}\n",
        "        ]\n",
        "    }\n",
        "\n",
        "# Convert DataFrames to Datasets\n",
        "train_dataset = Dataset.from_pandas(train_df)\n",
        "test_dataset = Dataset.from_pandas(test_df)\n",
        "\n",
        "# Apply conversion\n",
        "train_dataset = train_dataset.map(convert_to_messages, remove_columns=train_dataset.column_names)\n",
        "test_dataset = test_dataset.map(convert_to_messages, remove_columns=test_dataset.column_names)\n",
        "\n",
        "print(f\"Train dataset: {len(train_dataset)} samples\")\n",
        "print(f\"Test dataset: {len(test_dataset)} samples\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Preview a sample\n",
        "print(\"Sample training example:\")\n",
        "print(json.dumps(train_dataset[0], indent=2))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save datasets to JSONL files\n",
        "train_filename = os.path.join(dataset_parent_path, \"train_data.jsonl\")\n",
        "test_filename = os.path.join(dataset_parent_path, \"test_data.jsonl\")\n",
        "\n",
        "train_dataset.to_json(train_filename, lines=True)\n",
        "test_dataset.to_json(test_filename, lines=True)\n",
        "\n",
        "print(f\"Saved training data to: {train_filename}\")\n",
        "print(f\"Saved test data to: {test_filename}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Upload Training Data to S3\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sagemaker.s3 import S3Uploader\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "data_s3_uri = f\"s3://{sess.default_bucket()}/gemma-sentiment-finetune/dataset\"\n",
        "\n",
        "# Check if data already exists at this S3 location\n",
        "s3_client = boto3.client('s3')\n",
        "bucket = sess.default_bucket()\n",
        "prefix = \"gemma-sentiment-finetune/dataset/\"\n",
        "\n",
        "response = s3_client.list_objects_v2(Bucket=bucket, Prefix=prefix, MaxKeys=1)\n",
        "if response.get('KeyCount', 0) > 0:\n",
        "    existing_files = [obj['Key'] for obj in response.get('Contents', [])]\n",
        "    raise FileExistsError(\n",
        "        f\"S3 path already contains data!\\n\"\n",
        "        f\"Location: s3://{bucket}/{prefix}\\n\"\n",
        "        f\"Found: {existing_files}\\n\\n\"\n",
        "        f\"To overwrite, manually delete the existing data first:\\n\"\n",
        "        f\"  aws s3 rm s3://{bucket}/{prefix} --recursive\"\n",
        "    )\n",
        "\n",
        "uploaded_s3_uri = S3Uploader.upload(\n",
        "    local_path=train_filename,\n",
        "    desired_s3_uri=data_s3_uri\n",
        ")\n",
        "print(f\"Uploaded training data to: {uploaded_s3_uri}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Configure and Launch Training Job\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sagemaker.modules.configs import (\n",
        "    CheckpointConfig,\n",
        "    Compute,\n",
        "    InputData,\n",
        "    OutputDataConfig,\n",
        "    SourceCode,\n",
        "    StoppingCondition,\n",
        ")\n",
        "from sagemaker.modules.train import ModelTrainer\n",
        "from getpass import getpass\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "MODEL_ID = \"google/gemma-3-4b-it\"\n",
        "# Enter your HuggingFace token (required for gated models like Gemma)\n",
        "hf_token = getpass(\"Enter your HuggingFace token: \")\n",
        "# Metrics will be reported to tensorboard\n",
        "reports_to = \"tensorboard\"\n",
        "# Job name\n",
        "job_name = MODEL_ID.replace('/', '--').replace('.', '-')\n",
        "print(f\"job_name: {job_name}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Training environment variables\n",
        "training_env = {\n",
        "    \"HF_TOKEN\": hf_token,\n",
        "    \"FI_EFA_USE_DEVICE_RDMA\": \"1\",\n",
        "    \"NCCL_DEBUG\": \"INFO\",\n",
        "    \"NCCL_SOCKET_IFNAME\": \"eth0\",\n",
        "    \"FI_PROVIDER\": \"efa\",\n",
        "    \"NCCL_PROTO\": \"simple\",\n",
        "    \"NCCL_NET_GDR_LEVEL\": \"5\"\n",
        "}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%%writefile sagemaker_code/requirements.txt\n",
        "transformers==4.55.0\n",
        "peft==0.17.0\n",
        "accelerate==1.10.0\n",
        "bitsandbytes==0.46.1\n",
        "datasets==4.0.0\n",
        "deepspeed==0.16.4\n",
        "evaluate==0.4.5\n",
        "hf-transfer==0.1.8\n",
        "hf_xet\n",
        "liger-kernel==0.6.1\n",
        "lm-eval[api]==0.4.9\n",
        "kernels>=0.9.0\n",
        "mlflow\n",
        "safetensors>=0.6.2\n",
        "sagemaker==2.251.1\n",
        "sagemaker-mlflow==0.1.0\n",
        "sentencepiece==0.2.0\n",
        "scikit-learn==1.7.1\n",
        "tokenizers>=0.21.4\n",
        "triton\n",
        "trl==0.21.0\n",
        "py7zr\n",
        "nvidia-ml-py\n",
        "wandb\n",
        "git+https://github.com/triton-lang/triton.git@main#subdirectory=python/triton_kernels\n",
        "vllm==0.10.1\n",
        "poetry\n",
        "yq\n",
        "psutil\n",
        "nvidia-ml-py\n",
        "pyrsmi\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Training Configuration\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Training arguments for PEFT QLoRA\n",
        "args = [\n",
        "    \"--config\",\n",
        "    \"hf_recipes/google/gemma-3-4b-it--vanilla-peft-qlora.yaml\",\n",
        "]\n",
        "\n",
        "# Instance configuration\n",
        "training_instance_type = \"ml.g5.2xlarge\"\n",
        "training_instance_count = 1\n",
        "\n",
        "print(f\"Training instance: {training_instance_type} x {training_instance_count}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Get the PyTorch training image\n",
        "pytorch_image_uri = sagemaker.image_uris.retrieve(\n",
        "    framework=\"pytorch\",\n",
        "    region=sess.boto_session.region_name,\n",
        "    version=\"2.7.1\",\n",
        "    instance_type=training_instance_type,\n",
        "    image_scope=\"training\",\n",
        ")\n",
        "print(f\"Using image: {pytorch_image_uri}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Configure the ModelTrainer\n",
        "source_code = SourceCode(\n",
        "    source_dir=\"./sagemaker_code\",\n",
        "    command=f\"bash sm_accelerate_train.sh {' '.join(args)}\",\n",
        ")\n",
        "\n",
        "compute_configs = Compute(\n",
        "    instance_type=training_instance_type,\n",
        "    instance_count=training_instance_count,\n",
        "    keep_alive_period_in_seconds=1800,\n",
        "    volume_size_in_gb=300\n",
        ")\n",
        "\n",
        "base_job_name = f\"{job_name}-sentiment-finetune\"\n",
        "output_path = f\"s3://{sess.default_bucket()}/{base_job_name}\"\n",
        "\n",
        "model_trainer = ModelTrainer(\n",
        "    training_image=pytorch_image_uri,\n",
        "    source_code=source_code,\n",
        "    base_job_name=base_job_name,\n",
        "    compute=compute_configs,\n",
        "    stopping_condition=StoppingCondition(max_runtime_in_seconds=18000),\n",
        "    output_data_config=OutputDataConfig(\n",
        "        s3_output_path=output_path,\n",
        "    ),\n",
        "    checkpoint_config=CheckpointConfig(\n",
        "        s3_uri=os.path.join(\n",
        "            output_path,\n",
        "            \"sentiment-analysis\",\n",
        "            job_name,\n",
        "            \"checkpoints\"\n",
        "        ), \n",
        "        local_path=\"/opt/ml/checkpoints\"\n",
        "    ),\n",
        "    role=role,\n",
        "    environment=training_env\n",
        ")\n",
        "\n",
        "print(f\"base_job_name: {base_job_name}\")\n",
        "print(f\"output_path: {output_path}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Launch Training Job\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Launch the training job\n",
        "model_trainer.train(\n",
        "    input_data_config=[\n",
        "        InputData(\n",
        "            channel_name=\"training\",\n",
        "            data_source=uploaded_s3_uri,  \n",
        "        )\n",
        "    ], \n",
        "    wait=False  # Set to True to wait for completion\n",
        ")\n",
        "\n",
        "print(\"\\nTraining job launched!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Next Steps\n",
        "\n",
        "Once the training job completes:\n",
        "\n",
        "1. The fine-tuned model will be saved to `{output_path}/model/`\n",
        "2. Run notebook 3 to deploy the model and run evaluations against the test set (`test_data.jsonl`)\n",
        "\n",
        "**Test data location:** `tmp_cache_local_dataset/test_data.jsonl` (200 samples held out for evaluation)\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
