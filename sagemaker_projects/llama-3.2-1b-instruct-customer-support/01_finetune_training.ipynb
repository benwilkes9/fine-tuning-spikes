{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Fine-tune `meta-llama/Llama-3.2-1B-Instruct` for Customer Support Triage\n",
        "---\n",
        "This notebook fine-tunes **Llama-3.2-1B-Instruct** to transform customer support tickets into structured internal bug reports using Amazon SageMaker.\n",
        "\n",
        "**What this notebook does:**\n",
        "1. Load the source data \n",
        "2. Format and upload training data to S3\n",
        "3. Launch a SageMaker training job using LoRA (PEFT)\n",
        "\n",
        "---\n",
        "\n",
        "**Model:** [meta-llama/Llama-3.2-1B-Instruct](https://huggingface.co/meta-llama/Llama-3.2-1B-Instruct)  \n",
        "**Training Method:** PEFT LoRA (bf16 base with LoRA adapters)  \n",
        "**Instance:** ml.g5.xlarge (A10G 24GB)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Setup and Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import json\n",
        "import random\n",
        "from collections import defaultdict\n",
        "import boto3\n",
        "import sagemaker\n",
        "from sagemaker.s3 import S3Uploader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Set random seed for reproducibility\n",
        "random.seed(42)\n",
        "\n",
        "region = boto3.Session().region_name\n",
        "sess = sagemaker.Session(boto3.Session(region_name=region))\n",
        "\n",
        "sagemaker_session_bucket = None\n",
        "if sagemaker_session_bucket is None and sess is not None:\n",
        "    sagemaker_session_bucket = sess.default_bucket()\n",
        "\n",
        "try:\n",
        "    role = sagemaker.get_execution_role()\n",
        "except ValueError:\n",
        "    # Fallback to an explicit SageMaker execution role ARN if not using a SageMaker execution role\n",
        "    role = \"arn:aws:iam::889772146711:role/SageMakerExecutionRole\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(f\"sagemaker role arn: {role}\")\n",
        "print(f\"sagemaker bucket: {sess.default_bucket()}\")\n",
        "print(f\"sagemaker session region: {sess.boto_region_name}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Load and Preview Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create output directory\n",
        "dataset_parent_path = os.path.join(os.getcwd(), \"tmp_cache_local_dataset\")\n",
        "os.makedirs(dataset_parent_path, exist_ok=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load source data\n",
        "source_file = os.path.join(os.getcwd(), \"source_data.jsonl\")\n",
        "data = []\n",
        "with open(source_file, \"r\") as f:\n",
        "    for line in f:\n",
        "        line = line.strip()\n",
        "        if line:\n",
        "            data.append(json.loads(line))\n",
        "\n",
        "print(f\"Loaded {len(data)} records\")\n",
        "\n",
        "# Show category distribution\n",
        "category_counts = defaultdict(int)\n",
        "for entry in data:\n",
        "    category_counts[entry[\"category\"]] += 1\n",
        "\n",
        "print(\"\\nCategory distribution:\")\n",
        "for category, count in sorted(category_counts.items()):\n",
        "    print(f\"  {category}: {count}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Preview sample entry\n",
        "print(\"Sample entry:\")\n",
        "print(\"=\" * 60)\n",
        "sample = data[0]\n",
        "print(f\"Category: {sample['category']}\")\n",
        "for msg in sample[\"messages\"]:\n",
        "    content_preview = msg[\"content\"][:200] + \"...\" if len(msg[\"content\"]) > 200 else msg[\"content\"]\n",
        "    print(f\"{msg['role'].upper()}: {content_preview}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Format and Save Data\n",
        "\n",
        "Format all data for training. The SFT training script expects data in the `messages` format (without the category field):\n",
        "```json\n",
        "{\n",
        "  \"messages\": [\n",
        "    { \"role\": \"user\", \"content\": \"...\" },\n",
        "    { \"role\": \"assistant\", \"content\": \"...\" }\n",
        "  ]\n",
        "}\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Format for training: remove category field, keep only messages\n",
        "def format_for_training(entries):\n",
        "    return [{\"messages\": entry[\"messages\"]} for entry in entries]\n",
        "\n",
        "train_formatted = format_for_training(data)\n",
        "\n",
        "print(f\"Formatted {len(train_formatted)} samples for training\")\n",
        "print(\"\\nExample entry:\")\n",
        "print(json.dumps(train_formatted[0], indent=2)[:500] + \"...\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save training data to JSONL file\n",
        "train_filename = os.path.join(dataset_parent_path, \"train_data.jsonl\")\n",
        "\n",
        "with open(train_filename, \"w\") as f:\n",
        "    for entry in train_formatted:\n",
        "        f.write(json.dumps(entry) + \"\\n\")\n",
        "\n",
        "print(f\"Saved training data to: {train_filename}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Upload Training Data to S3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "data_s3_uri = f\"s3://{sess.default_bucket()}/llama-customer-support-finetune/dataset\"\n",
        "\n",
        "# Check if data already exists at this S3 location\n",
        "s3_client = boto3.client('s3')\n",
        "bucket = sess.default_bucket()\n",
        "prefix = \"llama-customer-support-finetune/dataset/\"\n",
        "\n",
        "response = s3_client.list_objects_v2(Bucket=bucket, Prefix=prefix, MaxKeys=1)\n",
        "if response.get('KeyCount', 0) > 0:\n",
        "    existing_files = [obj['Key'] for obj in response.get('Contents', [])]\n",
        "    raise FileExistsError(\n",
        "        f\"S3 path already contains data!\\n\"\n",
        "        f\"Location: s3://{bucket}/{prefix}\\n\"\n",
        "        f\"Found: {existing_files}\\n\\n\"\n",
        "        f\"To overwrite, manually delete the existing data first:\\n\"\n",
        "        f\"  aws s3 rm s3://{bucket}/{prefix} --recursive\"\n",
        "    )\n",
        "\n",
        "uploaded_s3_uri = S3Uploader.upload(\n",
        "    local_path=train_filename,\n",
        "    desired_s3_uri=data_s3_uri\n",
        ")\n",
        "print(f\"Uploaded training data to: {uploaded_s3_uri}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Configure Training Job"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sagemaker.modules.configs import (\n",
        "    CheckpointConfig,\n",
        "    Compute,\n",
        "    InputData,\n",
        "    OutputDataConfig,\n",
        "    SourceCode,\n",
        "    StoppingCondition,\n",
        ")\n",
        "from sagemaker.modules.train import ModelTrainer\n",
        "from getpass import getpass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Enter and validate HuggingFace token (required for gated models like Llama)\n",
        "hf_token = getpass(\"Enter your HuggingFace token: \")\n",
        "\n",
        "# Validate token format\n",
        "if not hf_token:\n",
        "    raise ValueError(\"❌ HuggingFace token cannot be empty!\")\n",
        "elif not hf_token.startswith(\"hf_\"):\n",
        "    raise ValueError(\n",
        "        f\"❌ Invalid HuggingFace token format!\\n\"\n",
        "        f\"   Token should start with 'hf_' but starts with '{hf_token[:3]}...'\\n\"\n",
        "        f\"   Get a valid token at: https://huggingface.co/settings/tokens\"\n",
        "    )\n",
        "elif len(hf_token) < 20:\n",
        "    raise ValueError(\n",
        "        f\"❌ HuggingFace token too short!\\n\"\n",
        "        f\"   Token is only {len(hf_token)} characters (expected 37+)\\n\"\n",
        "        f\"   Make sure you copied the full token from: https://huggingface.co/settings/tokens\"\n",
        "    )\n",
        "else:\n",
        "    print(f\"✓ HuggingFace token accepted\")\n",
        "    print(f\"  - Format: Valid (starts with 'hf_')\")\n",
        "    print(f\"  - Length: {len(hf_token)} characters\")\n",
        "    print(f\"  - Preview: {hf_token[:7]}...{hf_token[-4:]}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "MODEL_ID = \"meta-llama/Llama-3.2-1B-Instruct\"\n",
        "# Metrics will be reported to tensorboard\n",
        "reports_to = \"tensorboard\"\n",
        "# Job name\n",
        "job_name = MODEL_ID.replace('/', '--').replace('.', '-')\n",
        "print(f\"job_name: {job_name}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Training environment variables\n",
        "training_env = {\n",
        "    \"HF_TOKEN\": hf_token,\n",
        "    \"FI_EFA_USE_DEVICE_RDMA\": \"1\",\n",
        "    \"NCCL_DEBUG\": \"INFO\",\n",
        "    \"NCCL_SOCKET_IFNAME\": \"eth0\",\n",
        "    \"FI_PROVIDER\": \"efa\",\n",
        "    \"NCCL_PROTO\": \"simple\",\n",
        "    \"NCCL_NET_GDR_LEVEL\": \"5\"\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Training arguments for PEFT LoRA\n",
        "args = [\n",
        "    \"--config\",\n",
        "    \"hf_recipes/meta-llama/Llama-3.2-1B-Instruct--vanilla-peft-lora.yaml\",\n",
        "]\n",
        "\n",
        "# Instance configuration\n",
        "training_instance_type = \"ml.g5.xlarge\"  # A10G 1GPU 24GB\n",
        "training_instance_count = 1\n",
        "\n",
        "print(f\"Training instance: {training_instance_type} x {training_instance_count}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Get the PyTorch training image\n",
        "pytorch_image_uri = sagemaker.image_uris.retrieve(\n",
        "    framework=\"pytorch\",\n",
        "    region=sess.boto_session.region_name,\n",
        "    version=\"2.7.1\",\n",
        "    instance_type=training_instance_type,\n",
        "    image_scope=\"training\",\n",
        ")\n",
        "print(f\"Using image: {pytorch_image_uri}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Configure the ModelTrainer\n",
        "source_code = SourceCode(\n",
        "    source_dir=\"./sagemaker_code\",\n",
        "    command=f\"bash sm_accelerate_train.sh {' '.join(args)}\",\n",
        ")\n",
        "\n",
        "compute_configs = Compute(\n",
        "    instance_type=training_instance_type,\n",
        "    instance_count=training_instance_count,\n",
        "    keep_alive_period_in_seconds=1800,\n",
        "    volume_size_in_gb=125\n",
        ")\n",
        "\n",
        "import time\n",
        "timestamp = int(time.time())\n",
        "# Keep job name short - SageMaker has 63 char limit!\n",
        "base_job_name = f\"llama1b-cs-ft-{timestamp}\"\n",
        "output_path = f\"s3://{sess.default_bucket()}/{base_job_name}\"\n",
        "\n",
        "model_trainer = ModelTrainer(\n",
        "    training_image=pytorch_image_uri,\n",
        "    source_code=source_code,\n",
        "    base_job_name=base_job_name,\n",
        "    compute=compute_configs,\n",
        "    stopping_condition=StoppingCondition(max_runtime_in_seconds=18000),\n",
        "    output_data_config=OutputDataConfig(\n",
        "        s3_output_path=output_path,\n",
        "    ),\n",
        "    checkpoint_config=CheckpointConfig(\n",
        "        s3_uri=os.path.join(\n",
        "            output_path,\n",
        "            \"customer-support\",\n",
        "            job_name,\n",
        "            \"checkpoints\"\n",
        "        ), \n",
        "        local_path=\"/opt/ml/checkpoints\"\n",
        "    ),\n",
        "    role=role,\n",
        "    environment=training_env\n",
        ")\n",
        "\n",
        "print(f\"base_job_name: {base_job_name}\")\n",
        "print(f\"output_path: {output_path}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## 6. Launch Training Job"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Launch the training job\n",
        "model_trainer.train(\n",
        "    input_data_config=[\n",
        "        InputData(\n",
        "            channel_name=\"training\",\n",
        "            data_source=uploaded_s3_uri,  \n",
        "        )\n",
        "    ], \n",
        "    wait=False  # Set to True to wait for completion\n",
        ")\n",
        "\n",
        "print(\"\\nTraining job launched!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Wait for training to complete and stream logs\n",
        "training_job = model_trainer._latest_training_job\n",
        "print(f\"Training job name: {training_job.training_job_name}\")\n",
        "print(\"Waiting for training to complete...\\n\")\n",
        "training_job.wait(logs=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Next Steps\n",
        "\n",
        "Once the training job completes:\n",
        "\n",
        "1. The fine-tuned model will be saved to `{output_path}/model/`\n",
        "2. Run notebook 2 (`02_deploy_and_evaluate.ipynb`) to deploy the model and demo it"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
