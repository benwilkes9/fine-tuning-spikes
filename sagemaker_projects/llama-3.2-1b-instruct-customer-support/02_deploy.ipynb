{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Deploy Fine-Tuned Llama-3.2-1B for Customer Support Triage\n",
        "---\n",
        "This notebook deploys the fine-tuned **Llama-3.2-1B-Instruct** model from notebook 1 to a SageMaker endpoint.\n",
        "\n",
        "**Prerequisites:**\n",
        "- Run notebook 1 (`01_finetune_training.ipynb`) to fine-tune the model\n",
        "- Training job must be completed successfully\n",
        "\n",
        "**What this notebook does:**\n",
        "1. Extract model artifacts from S3\n",
        "2. Deploy model to SageMaker endpoint using DJL LMI (vLLM)\n",
        "3. Run quick sanity check\n",
        "4. Provide cleanup instructions\n",
        "\n",
        "**Next step:** Run notebook 3 (`03_evaluate.ipynb`) for full evaluation\n",
        "\n",
        "---\n",
        "\n",
        "**Model:** [meta-llama/Llama-3.2-1B-Instruct](https://huggingface.co/meta-llama/Llama-3.2-1B-Instruct)  \n",
        "**Instance:** ml.g4dn.xlarge (1x T4, 16GB VRAM)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Setup and Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import boto3\n",
        "import sagemaker\n",
        "from sagemaker.session import Session\n",
        "import json\n",
        "import time\n",
        "from getpass import getpass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Configuration\n",
        "region = \"eu-west-2\"\n",
        "role = \"arn:aws:iam::889772146711:role/SageMakerExecutionRole\"\n",
        "s3_bucket = \"sagemaker-eu-west-2-889772146711\"\n",
        "\n",
        "# UPDATE THIS after running notebook 01\n",
        "training_job_name = \"llama1b-cs-ft-1768325350-20260113172916\"\n",
        "\n",
        "# S3 paths (matches notebook 01 output structure)\n",
        "base_job_name = \"llama1b-cs-ft-1768325350\"\n",
        "s3_model_uri = f\"s3://{s3_bucket}/{base_job_name}/{training_job_name}/output/model.tar.gz\"\n",
        "s3_uncompressed_prefix = f\"{base_job_name}/{training_job_name}/output/uncompressed/\"\n",
        "s3_model_prefix = f\"s3://{s3_bucket}/{s3_uncompressed_prefix}\"\n",
        "\n",
        "# Create clients\n",
        "sess = Session(boto3.Session(region_name=region))\n",
        "sm_client = boto3.client(\"sagemaker\", region_name=region)\n",
        "s3_client = boto3.client(\"s3\", region_name=region)\n",
        "sagemaker_runtime = boto3.client(\"sagemaker-runtime\", region_name=region)\n",
        "\n",
        "print(f\"Training job: {training_job_name}\")\n",
        "print(f\"Model artifacts: {s3_model_uri}\")\n",
        "print(f\"Uncompressed path: {s3_model_prefix}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Enter and validate HuggingFace token (required for gated models like Llama)\n",
        "hf_token = getpass(\"Enter your HuggingFace token: \")\n",
        "\n",
        "# Validate token format\n",
        "if not hf_token:\n",
        "    raise ValueError(\"❌ HuggingFace token cannot be empty!\")\n",
        "elif not hf_token.startswith(\"hf_\"):\n",
        "    raise ValueError(\n",
        "        f\"❌ Invalid HuggingFace token format!\\n\"\n",
        "        f\"   Token should start with 'hf_' but starts with '{hf_token[:3]}...'\\n\"\n",
        "        f\"   Get a valid token at: https://huggingface.co/settings/tokens\"\n",
        "    )\n",
        "elif len(hf_token) < 20:\n",
        "    raise ValueError(\n",
        "        f\"❌ HuggingFace token too short!\\n\"\n",
        "        f\"   Token is only {len(hf_token)} characters (expected 37+)\\n\"\n",
        "        f\"   Make sure you copied the full token from: https://huggingface.co/settings/tokens\"\n",
        "    )\n",
        "else:\n",
        "    print(f\"✓ HuggingFace token accepted\")\n",
        "    print(f\"  - Format: Valid (starts with 'hf_')\")\n",
        "    print(f\"  - Length: {len(hf_token)} characters\")\n",
        "    print(f\"  - Preview: {hf_token[:7]}...{hf_token[-4:]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Prepare Model Artifacts\n",
        "\n",
        "Extract `model.tar.gz` to S3 (streaming, no local download)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%%time\n",
        "import tarfile\n",
        "from smart_open import open as smart_open\n",
        "\n",
        "# Check if already extracted\n",
        "response = s3_client.list_objects_v2(\n",
        "    Bucket=s3_bucket, \n",
        "    Prefix=s3_uncompressed_prefix + \"meta-llama/Llama-3.2-1B-Instruct/\",\n",
        "    MaxKeys=5\n",
        ")\n",
        "already_extracted = response.get(\"KeyCount\", 0) > 0\n",
        "\n",
        "# Extract if needed\n",
        "if not already_extracted:\n",
        "    print(\"Extracting model.tar.gz to S3 (streaming - no local download)...\")\n",
        "    print(\"This may take 5-10 minutes.\")\n",
        "    \n",
        "    file_count = 0\n",
        "    total_bytes = 0\n",
        "    \n",
        "    with smart_open(s3_model_uri, \"rb\") as f:\n",
        "        with tarfile.open(fileobj=f, mode=\"r:*\") as tar:\n",
        "            for member in tar:\n",
        "                if member.isfile():\n",
        "                    file_obj = tar.extractfile(member)\n",
        "                    if file_obj:\n",
        "                        content = file_obj.read()\n",
        "                        s3_client.put_object(\n",
        "                            Bucket=s3_bucket,\n",
        "                            Key=s3_uncompressed_prefix + member.name,\n",
        "                            Body=content\n",
        "                        )\n",
        "                        file_count += 1\n",
        "                        total_bytes += len(content)\n",
        "                        if file_count % 10 == 0:\n",
        "                            print(f\"  Extracted {file_count} files ({total_bytes / 1e9:.2f} GB)...\")\n",
        "    \n",
        "    print(f\"Extraction complete: {file_count} files, {total_bytes / 1e9:.2f} GB\")\n",
        "else:\n",
        "    print(\"Model already extracted. Skipping.\")\n",
        "\n",
        "print(f\"Model artifacts ready at: {s3_model_prefix}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Create SageMaker Resources\n",
        "\n",
        "Deploy using DJL LMI container with vLLM backend."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Resource names\n",
        "model_name = sagemaker.utils.name_from_base(\"llama-3-2-1b-customer-support\")\n",
        "endpoint_config_name = f\"epc-{model_name}\"\n",
        "endpoint_name = f\"ep-{model_name}\"\n",
        "inference_component_name = f\"ic-{model_name}\"\n",
        "variant_name = \"AllTraffic\"\n",
        "instance_type = \"ml.g4dn.xlarge\"  # 1x T4 (16GB) - sufficient for 1B model\n",
        "num_gpu = 1\n",
        "\n",
        "print(f\"Model: {model_name}\")\n",
        "print(f\"Endpoint: {endpoint_name}\")\n",
        "print(f\"Inference Component: {inference_component_name}\")\n",
        "print(f\"Instance: {instance_type}\")\n",
        "print(\"\\n⚠️  SAVE THESE NAMES - You'll need them for notebook 03!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create Model\n",
        "inference_image = f\"763104351884.dkr.ecr.{region}.amazonaws.com/djl-inference:0.34.0-lmi16.0.0-cu128\"\n",
        "\n",
        "sm_client.create_model(\n",
        "    ModelName=model_name,\n",
        "    ExecutionRoleArn=role,\n",
        "    PrimaryContainer={\n",
        "        \"Image\": inference_image,\n",
        "        \"ModelDataSource\": {\n",
        "            \"S3DataSource\": {\n",
        "                \"S3Uri\": s3_model_prefix,\n",
        "                \"S3DataType\": \"S3Prefix\",\n",
        "                \"CompressionType\": \"None\",\n",
        "            }\n",
        "        },\n",
        "        \"Environment\": {\n",
        "            \"HF_TOKEN\": hf_token,\n",
        "            \"SAGEMAKER_MODEL_SERVER_WORKERS\": \"1\",\n",
        "            \"MESSAGES_API_ENABLED\": \"true\",\n",
        "            \"OPTION_MAX_ROLLING_BATCH_SIZE\": \"8\",\n",
        "            \"OPTION_MODEL_LOADING_TIMEOUT\": \"1500\",\n",
        "            \"SERVING_FAIL_FAST\": \"true\",\n",
        "            \"OPTION_ROLLING_BATCH\": \"disable\",\n",
        "            \"OPTION_ASYNC_MODE\": \"true\",\n",
        "            \"OPTION_ENTRYPOINT\": \"djl_python.lmi_vllm.vllm_async_service\",\n",
        "            \"OPTION_ENABLE_STREAMING\": \"true\",\n",
        "            \"MAX_TOTAL_TOKENS\": \"4096\",\n",
        "            # T4 doesn't support FA2; use xformers backend\n",
        "            \"VLLM_ATTENTION_BACKEND\": \"XFORMERS\",\n",
        "        },\n",
        "    },\n",
        ")\n",
        "print(f\"Created model: {model_name}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create Endpoint Configuration\n",
        "sm_client.create_endpoint_config(\n",
        "    EndpointConfigName=endpoint_config_name,\n",
        "    ExecutionRoleArn=role,\n",
        "    ProductionVariants=[{\n",
        "        \"VariantName\": variant_name,\n",
        "        \"InstanceType\": instance_type,\n",
        "        \"InitialInstanceCount\": 1,\n",
        "        \"ModelDataDownloadTimeoutInSeconds\": 3600,\n",
        "        \"ContainerStartupHealthCheckTimeoutInSeconds\": 3600,\n",
        "        \"ManagedInstanceScaling\": {\"Status\": \"ENABLED\", \"MinInstanceCount\": 1, \"MaxInstanceCount\": 1},\n",
        "        \"RoutingConfig\": {\"RoutingStrategy\": \"LEAST_OUTSTANDING_REQUESTS\"},\n",
        "    }],\n",
        ")\n",
        "print(f\"Created endpoint config: {endpoint_config_name}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Deploy Endpoint\n",
        "sm_client.create_endpoint(EndpointName=endpoint_name, EndpointConfigName=endpoint_config_name)\n",
        "print(f\"Creating endpoint: {endpoint_name}\")\n",
        "print(\"Waiting for endpoint (this may take several minutes)...\")\n",
        "\n",
        "sess.wait_for_endpoint(endpoint_name)\n",
        "print(f\"Endpoint {endpoint_name} is InService!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create Inference Component\n",
        "sm_client.create_inference_component(\n",
        "    InferenceComponentName=inference_component_name,\n",
        "    EndpointName=endpoint_name,\n",
        "    VariantName=variant_name,\n",
        "    Specification={\n",
        "        \"ModelName\": model_name,\n",
        "        \"ComputeResourceRequirements\": {\n",
        "            \"NumberOfAcceleratorDevicesRequired\": num_gpu,\n",
        "            \"NumberOfCpuCoresRequired\": 1,\n",
        "            \"MinMemoryRequiredInMb\": 1024,\n",
        "        },\n",
        "    },\n",
        "    RuntimeConfig={\"CopyCount\": 1},\n",
        ")\n",
        "print(f\"Creating inference component: {inference_component_name}\")\n",
        "print(\"Waiting for inference component (this may take ~10 minutes)...\")\n",
        "\n",
        "start_time = time.time()\n",
        "while True:\n",
        "    desc = sm_client.describe_inference_component(InferenceComponentName=inference_component_name)\n",
        "    status = desc[\"InferenceComponentStatus\"]\n",
        "    if status in [\"InService\", \"Failed\"]:\n",
        "        break\n",
        "    print(f\"  Status: {status}\")\n",
        "    time.sleep(30)\n",
        "\n",
        "if status == \"Failed\":\n",
        "    raise Exception(f\"Inference component failed: {desc.get('FailureReason', 'Unknown')}\")\n",
        "\n",
        "print(f\"\\nInference component ready! ({(time.time() - start_time)/60:.1f} minutes)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Test Inference\n",
        "\n",
        "Quick sanity check before running full evaluation in notebook 03."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def invoke_model(messages, max_tokens=512, temperature=0.1):\n",
        "    \"\"\"Invoke the deployed model with messages.\"\"\"\n",
        "    payload = {\n",
        "        \"messages\": messages,\n",
        "        \"temperature\": temperature,\n",
        "        \"top_p\": 0.9,\n",
        "        \"max_tokens\": max_tokens,\n",
        "    }\n",
        "    \n",
        "    response = sagemaker_runtime.invoke_endpoint(\n",
        "        EndpointName=endpoint_name,\n",
        "        InferenceComponentName=inference_component_name,\n",
        "        ContentType=\"application/json\",\n",
        "        Body=json.dumps(payload)\n",
        "    )\n",
        "    \n",
        "    result = json.loads(response[\"Body\"].read().decode(\"utf-8\"))\n",
        "    return result[\"choices\"][0][\"message\"][\"content\"].strip()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Quick test with a sample customer support ticket\n",
        "test_messages = [\n",
        "    {\"role\": \"user\", \"content\": \"\"\"Ticket #9999\n",
        "Customer: test.user@example.com\n",
        "Plan: Enterprise\n",
        "Issue: URGENT - Users can't log in to the dashboard. Getting 401 errors across the board. Our sales team is blocked.\"\"\"}\n",
        "]\n",
        "\n",
        "response = invoke_model(test_messages)\n",
        "print(\"Test Response:\")\n",
        "print(\"=\" * 60)\n",
        "print(response)\n",
        "print(\"\\n✓ Endpoint is working! Proceed to notebook 03 for full evaluation.\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
