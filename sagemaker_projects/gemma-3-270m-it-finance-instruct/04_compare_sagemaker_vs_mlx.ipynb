{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Compare: SageMaker Fine-Tuned vs MLX Base Model\n",
        "---\n",
        "\n",
        "This notebook performs **side-by-side comparison** between:\n",
        "\n",
        "1. **Base Model (Local MLX)**: `gemma-3-270m-it-bf16` running on Apple Silicon\n",
        "2. **Fine-Tuned Model (SageMaker)**: Model fine-tuned on financial sentiment data\n",
        "\n",
        "**Purpose**: Evaluate how fine-tuning improves sentiment classification accuracy.\n",
        "\n",
        "**Prerequisites**:\n",
        "- Run notebook 03 to deploy the fine-tuned model endpoint (keep it running)\n",
        "- Have the endpoint and inference component names ready\n",
        "\n",
        "---\n",
        "\n",
        "**Test Data**: 600 samples from `tmp_cache_local_dataset/test_data.jsonl`  \n",
        "**Labels**: positive, negative, neutral, bullish, bearish"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Setup and Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import json\n",
        "import os\n",
        "import time\n",
        "import boto3\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from tqdm import tqdm\n",
        "from mlx_lm import load, generate\n",
        "from mlx_lm.sample_utils import make_sampler\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "\n",
        "print(\"Setup complete!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# CONFIGURATION - Update these values from notebook 03\n",
        "# =============================================================================\n",
        "\n",
        "# SageMaker endpoint (from notebook 03 - must be running)\n",
        "MODEL_NAME = \"gemma-3-270m-sentiment-2026-01-09-07-37-13-218\"  # UPDATE THIS\n",
        "SAGEMAKER_ENDPOINT_NAME = f\"ep-{MODEL_NAME}\"\n",
        "INFERENCE_COMPONENT_NAME = f\"ic-{MODEL_NAME}\"\n",
        "AWS_REGION = \"eu-west-2\"\n",
        "\n",
        "# Local MLX base model\n",
        "MLX_MODEL_ID = \"mlx-community/gemma-3-270m-it-bf16\"\n",
        "\n",
        "# Test data\n",
        "TEST_DATA_PATH = os.path.join(os.getcwd(), \"tmp_cache_local_dataset\", \"test_data.jsonl\")\n",
        "\n",
        "# Evaluation settings\n",
        "MAX_SAMPLES = None  # Set to None to use all 600 samples\n",
        "MAX_TOKENS = 32\n",
        "TEMPERATURE = 0.1\n",
        "TOP_P = 0.9\n",
        "\n",
        "print(f\"SageMaker Endpoint: {SAGEMAKER_ENDPOINT_NAME}\")\n",
        "print(f\"MLX Model: {MLX_MODEL_ID}\")\n",
        "print(f\"Test Data: {TEST_DATA_PATH}\")\n",
        "print(f\"Max Samples: {MAX_SAMPLES or 'All'}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Load Models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load local MLX base model\n",
        "print(f\"Loading local model: {MLX_MODEL_ID}\")\n",
        "print(\"This may take a moment on first run (downloading model weights)...\")\n",
        "\n",
        "mlx_model, mlx_tokenizer = load(MLX_MODEL_ID)\n",
        "\n",
        "print(\"Local MLX model loaded successfully!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Configure SageMaker client\n",
        "sagemaker_runtime = boto3.client(\"sagemaker-runtime\", region_name=AWS_REGION)\n",
        "\n",
        "print(f\"SageMaker client configured for region: {AWS_REGION}\")\n",
        "print(f\"Endpoint: {SAGEMAKER_ENDPOINT_NAME}\")\n",
        "print(f\"Inference Component: {INFERENCE_COMPONENT_NAME}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Inference Functions (Like-for-Like Comparison)\n",
        "\n",
        "Both inference functions use:\n",
        "- **Same input format**: messages list from test data\n",
        "- **Same label extraction**: shared `extract_label()` function\n",
        "- **Same valid labels**: positive, negative, neutral, bullish, bearish"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def generate_local_mlx(messages: list) -> str:\n",
        "    \"\"\"\n",
        "    Generate prediction from local MLX base model.\n",
        "    \n",
        "    Args:\n",
        "        messages: List of message dicts (system + user only, no assistant)\n",
        "        \n",
        "    Returns:\n",
        "        Extracted label from model response\n",
        "    \"\"\"\n",
        "    # Apply chat template (system + user messages only)\n",
        "    input_messages = [m for m in messages if m[\"role\"] != \"assistant\"]\n",
        "    \n",
        "    prompt = mlx_tokenizer.apply_chat_template(\n",
        "        input_messages,\n",
        "        tokenize=False,\n",
        "        add_generation_prompt=True\n",
        "    )\n",
        "\n",
        "    # Create sampler with temperature and top_p (matching SageMaker config)\n",
        "    sampler = make_sampler(temp=TEMPERATURE, top_p=TOP_P)\n",
        "    \n",
        "    # Generate response\n",
        "    response = generate(\n",
        "        mlx_model,\n",
        "        mlx_tokenizer,\n",
        "        prompt=prompt,\n",
        "        max_tokens=MAX_TOKENS,\n",
        "        verbose=False,\n",
        "        sampler=sampler\n",
        "    )\n",
        "\n",
        "    return response\n",
        "\n",
        "print(\"MLX inference function defined.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def generate_sagemaker(messages: list) -> str:\n",
        "    \"\"\"\n",
        "    Generate prediction from fine-tuned model on SageMaker.\n",
        "    \n",
        "    Args:\n",
        "        messages: List of message dicts (system + user only, no assistant)\n",
        "        \n",
        "    Returns:\n",
        "        Extracted label from model response\n",
        "    \"\"\"\n",
        "    # Use only system + user messages\n",
        "    input_messages = [m for m in messages if m[\"role\"] != \"assistant\"]\n",
        "    \n",
        "    payload = {\n",
        "        \"messages\": input_messages,\n",
        "        \"temperature\": TEMPERATURE,\n",
        "        \"top_p\": TOP_P,\n",
        "        \"max_tokens\": MAX_TOKENS,\n",
        "    }\n",
        "    \n",
        "    response = sagemaker_runtime.invoke_endpoint(\n",
        "        EndpointName=SAGEMAKER_ENDPOINT_NAME,\n",
        "        InferenceComponentName=INFERENCE_COMPONENT_NAME,\n",
        "        ContentType=\"application/json\",\n",
        "        Body=json.dumps(payload)\n",
        "    )\n",
        "    \n",
        "    result = json.loads(response[\"Body\"].read().decode(\"utf-8\"))\n",
        "    response_text = result[\"choices\"][0][\"message\"][\"content\"].strip()\n",
        "    \n",
        "    return response_text\n",
        "\n",
        "print(\"SageMaker inference function defined.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Run Evaluation Loop\n",
        "\n",
        "Run both models on the test dataset and collect predictions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load test data\n",
        "test_samples = []\n",
        "with open(TEST_DATA_PATH, \"r\") as f:\n",
        "    for line in f:\n",
        "        test_samples.append(json.loads(line))\n",
        "\n",
        "# Apply sample limit if set\n",
        "if MAX_SAMPLES:\n",
        "    test_samples = test_samples[:MAX_SAMPLES]\n",
        "\n",
        "print(f\"Loaded {len(test_samples)} test samples\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Run evaluation on both models\n",
        "ground_truth = []\n",
        "mlx_predictions = []\n",
        "sagemaker_predictions = []\n",
        "texts = []\n",
        "\n",
        "print(f\"Running inference on {len(test_samples)} samples...\")\n",
        "print(\"This will run each sample through both MLX (local) and SageMaker (remote).\\n\")\n",
        "\n",
        "for i, sample in enumerate(tqdm(test_samples, desc=\"Evaluating\")):\n",
        "    messages = sample[\"messages\"]\n",
        "    \n",
        "    # Extract ground truth (assistant message)\n",
        "    expected = messages[-1][\"content\"].strip().lower()\n",
        "    ground_truth.append(expected)\n",
        "    \n",
        "    # Extract text for display\n",
        "    user_text = messages[1][\"content\"] if len(messages) > 1 else \"\"\n",
        "    texts.append(user_text)\n",
        "    \n",
        "    # Get MLX prediction\n",
        "    try:\n",
        "        mlx_pred = generate_local_mlx(messages)\n",
        "    except Exception as e:\n",
        "        mlx_pred = \"error\"\n",
        "        print(f\"\\nMLX error on sample {i}: {e}\")\n",
        "    mlx_predictions.append(mlx_pred)\n",
        "    \n",
        "    # Get SageMaker prediction\n",
        "    try:\n",
        "        sm_pred = generate_sagemaker(messages)\n",
        "    except Exception as e:\n",
        "        sm_pred = \"error\"\n",
        "        print(f\"\\nSageMaker error on sample {i}: {e}\")\n",
        "    sagemaker_predictions.append(sm_pred)\n",
        "    \n",
        "    # Small delay for SageMaker rate limiting\n",
        "    time.sleep(0.1)\n",
        "\n",
        "print(f\"\\nCompleted: {len(ground_truth)} samples evaluated\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Results Comparison\n",
        "\n",
        "Compare accuracy, classification metrics, and confusion matrices for both models."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Calculate accuracies\n",
        "mlx_correct = [g == p for g, p in zip(ground_truth, mlx_predictions)]\n",
        "sm_correct = [g == p for g, p in zip(ground_truth, sagemaker_predictions)]\n",
        "\n",
        "mlx_accuracy = sum(mlx_correct) / len(mlx_correct)\n",
        "sm_accuracy = sum(sm_correct) / len(sm_correct)\n",
        "\n",
        "# Summary table\n",
        "print(\"=\" * 60)\n",
        "print(\"ACCURACY COMPARISON\")\n",
        "print(\"=\" * 60)\n",
        "print(f\"{'Model':<30} {'Accuracy':>15} {'Correct':>12}\")\n",
        "print(\"-\" * 60)\n",
        "print(f\"{'MLX Base (Local)':<30} {mlx_accuracy:>14.2%} {sum(mlx_correct):>8}/{len(ground_truth)}\")\n",
        "print(f\"{'SageMaker Fine-Tuned':<30} {sm_accuracy:>14.2%} {sum(sm_correct):>8}/{len(ground_truth)}\")\n",
        "print(\"-\" * 60)\n",
        "\n",
        "improvement = sm_accuracy - mlx_accuracy\n",
        "if improvement > 0:\n",
        "    print(f\"{'Fine-tuning improvement:':<30} {improvement:>+14.2%}\")\n",
        "elif improvement < 0:\n",
        "    print(f\"{'Fine-tuning change:':<30} {improvement:>+14.2%}\")\n",
        "else:\n",
        "    print(f\"{'No change in accuracy':<30}\")\n",
        "print(\"=\" * 60)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Per-sample results DataFrame\n",
        "results_df = pd.DataFrame({\n",
        "    \"text\": [t for t in texts],\n",
        "    \"expected\": ground_truth,\n",
        "    \"base_predicted\": mlx_predictions,\n",
        "    \"sagemaker_predicted\": sagemaker_predictions,\n",
        "    \"base_correct\": mlx_correct,\n",
        "    \"sagemaker_correct\": sm_correct,\n",
        "})\n",
        "\n",
        "results_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "VALID_LABELS = {'positive', 'negative', 'neutral', 'bullish', 'bearish'}\n",
        "\n",
        "# Classification reports\n",
        "labels = sorted(list(VALID_LABELS))\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"CLASSIFICATION REPORT: MLX Base Model (Local)\")\n",
        "print(\"=\" * 60)\n",
        "print(classification_report(ground_truth, mlx_predictions, labels=labels, zero_division=0))\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"CLASSIFICATION REPORT: SageMaker Fine-Tuned Model\")\n",
        "print(\"=\" * 60)\n",
        "print(classification_report(ground_truth, sagemaker_predictions, labels=labels, zero_division=0))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Side-by-side confusion matrices\n",
        "all_labels = sorted(list(set(ground_truth + mlx_predictions + sagemaker_predictions)))\n",
        "\n",
        "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "# MLX confusion matrix\n",
        "cm_mlx = confusion_matrix(ground_truth, mlx_predictions, labels=all_labels)\n",
        "sns.heatmap(cm_mlx, annot=True, fmt='d', cmap='Blues',\n",
        "            xticklabels=all_labels, yticklabels=all_labels, ax=axes[0])\n",
        "axes[0].set_xlabel('Predicted')\n",
        "axes[0].set_ylabel('Actual')\n",
        "axes[0].set_title(f'MLX Base Model (Local)\\nAccuracy: {mlx_accuracy:.2%}')\n",
        "\n",
        "# SageMaker confusion matrix\n",
        "cm_sm = confusion_matrix(ground_truth, sagemaker_predictions, labels=all_labels)\n",
        "sns.heatmap(cm_sm, annot=True, fmt='d', cmap='Greens',\n",
        "            xticklabels=all_labels, yticklabels=all_labels, ax=axes[1])\n",
        "axes[1].set_xlabel('Predicted')\n",
        "axes[1].set_ylabel('Actual')\n",
        "axes[1].set_title(f'SageMaker Fine-Tuned\\nAccuracy: {sm_accuracy:.2%}')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
