{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Fine-tune `google/gemma-3-270m-it` for Financial Sentiment Analysis\n",
        "---\n",
        "This notebook fine-tunes **Gemma-3-270M-IT** on a financial sentiment analysis dataset using Amazon SageMaker.\n",
        "\n",
        "**Prerequisites:**\n",
        "- Run notebook 1 (`01_data_analysis.ipynb`) first to generate `sentiment_training_data.csv`\n",
        "\n",
        "**What this notebook does:**\n",
        "1. Load the sentiment training data from notebook 1\n",
        "2. Split into 90% training / 10% test sets\n",
        "3. Convert to messages format (JSONL)\n",
        "4. Upload training data to S3\n",
        "5. Launch a SageMaker training job using LoRA (PEFT)\n",
        "\n",
        "---\n",
        "\n",
        "**Model:** [google/gemma-3-270m-it](https://huggingface.co/google/gemma-3-270m-it)  \n",
        "**Training Method:** PEFT LoRA (bf16 base with LoRA adapters)  \n",
        "**Instance:** ml.g5.xlarge\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Setup and Dependencies\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "sagemaker.config INFO - Not applying SDK defaults from location: /Library/Application Support/sagemaker/config.yaml\n",
            "sagemaker.config INFO - Not applying SDK defaults from location: /Users/ben/Library/Application Support/sagemaker/config.yaml\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import json\n",
        "import pandas as pd\n",
        "import boto3\n",
        "import sagemaker\n",
        "from datasets import Dataset\n",
        "from sklearn.model_selection import train_test_split\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Couldn't call 'get_role' to get Role ARN from role name ben.wilkes to get Role path.\n"
          ]
        }
      ],
      "source": [
        "region = boto3.Session().region_name\n",
        "\n",
        "sess = sagemaker.Session(boto3.Session(region_name=region))\n",
        "\n",
        "sagemaker_session_bucket = None\n",
        "if sagemaker_session_bucket is None and sess is not None:\n",
        "    sagemaker_session_bucket = sess.default_bucket()\n",
        "\n",
        "try:\n",
        "    role = sagemaker.get_execution_role()\n",
        "except ValueError:\n",
        "    # Fallback to an explicit SageMaker execution role ARN if not using a SageMaker execution role\n",
        "    role = \"arn:aws:iam::889772146711:role/SageMakerExecutionRole\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "sagemaker role arn: arn:aws:iam::889772146711:role/SageMakerExecutionRole\n",
            "sagemaker bucket: sagemaker-eu-west-2-889772146711\n",
            "sagemaker session region: eu-west-2\n"
          ]
        }
      ],
      "source": [
        "print(f\"sagemaker role arn: {role}\")\n",
        "print(f\"sagemaker bucket: {sess.default_bucket()}\")\n",
        "print(f\"sagemaker session region: {sess.boto_region_name}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Data Preparation\n",
        "\n",
        "Load the sentiment training data from notebook 1, split into train/test, and convert to messages format.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create output directory\n",
        "dataset_parent_path = os.path.join(os.getcwd(), \"tmp_cache_local_dataset\")\n",
        "os.makedirs(dataset_parent_path, exist_ok=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loaded 6000 samples from sentiment_training_data.csv\n",
            "\n",
            "Columns: ['system', 'user', 'assistant']\n",
            "\n",
            "Label distribution:\n",
            "assistant\n",
            "neutral     1500\n",
            "positive    1500\n",
            "negative    1500\n",
            "bullish     1500\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ],
      "source": [
        "# Load sentiment training data from notebook 1\n",
        "csv_path = os.path.join(os.getcwd(), \"sentiment_training_data.csv\")\n",
        "df = pd.read_csv(csv_path)\n",
        "\n",
        "print(f\"Loaded {len(df)} samples from sentiment_training_data.csv\")\n",
        "print(f\"\\nColumns: {list(df.columns)}\")\n",
        "print(f\"\\nLabel distribution:\")\n",
        "print(df['assistant'].value_counts())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training samples: 5400 (90%)\n",
            "Test samples: 600 (10%)\n",
            "\n",
            "Training label distribution:\n",
            "assistant\n",
            "negative    1350\n",
            "positive    1350\n",
            "neutral     1350\n",
            "bullish     1350\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ],
      "source": [
        "# Split data: 90% training, 10% test\n",
        "# The training data will be further split internally by sft.py (90/10) for train/eval\n",
        "train_df, test_df = train_test_split(\n",
        "    df, \n",
        "    test_size=0.10, \n",
        "    random_state=42, \n",
        "    stratify=df['assistant']  # Maintain label distribution\n",
        ")\n",
        "\n",
        "print(f\"Training samples: {len(train_df)} (90%)\")\n",
        "print(f\"Test samples: {len(test_df)} (10%)\")\n",
        "print(f\"\\nTraining label distribution:\")\n",
        "print(train_df['assistant'].value_counts())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Convert to Messages Format\n",
        "\n",
        "The SFT training script expects data in the `messages` format:\n",
        "```json\n",
        "{\n",
        "  \"messages\": [\n",
        "    { \"role\": \"system\", \"content\": \"...\" },\n",
        "    { \"role\": \"user\", \"content\": \"...\" },\n",
        "    { \"role\": \"assistant\", \"content\": \"...\" }\n",
        "  ]\n",
        "}\n",
        "```\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "7ce291e506de466797f9476ac62027de",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/5400 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "3ad929b8b571456e80aa761167babb31",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/600 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train dataset: 5400 samples\n",
            "Test dataset: 600 samples\n"
          ]
        }
      ],
      "source": [
        "def convert_to_messages(row):\n",
        "    \"\"\"Convert a row to messages format.\"\"\"\n",
        "    return {\n",
        "        \"messages\": [\n",
        "            {\"role\": \"system\", \"content\": row[\"system\"]},\n",
        "            {\"role\": \"user\", \"content\": row[\"user\"]},\n",
        "            {\"role\": \"assistant\", \"content\": row[\"assistant\"]}\n",
        "        ]\n",
        "    }\n",
        "\n",
        "# Convert DataFrames to Datasets\n",
        "train_dataset = Dataset.from_pandas(train_df)\n",
        "test_dataset = Dataset.from_pandas(test_df)\n",
        "\n",
        "# Apply conversion\n",
        "train_dataset = train_dataset.map(convert_to_messages, remove_columns=train_dataset.column_names)\n",
        "test_dataset = test_dataset.map(convert_to_messages, remove_columns=test_dataset.column_names)\n",
        "\n",
        "print(f\"Train dataset: {len(train_dataset)} samples\")\n",
        "print(f\"Test dataset: {len(test_dataset)} samples\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sample training example:\n",
            "{\n",
            "  \"messages\": [\n",
            "    {\n",
            "      \"content\": \"You are a financial sentiment analysis expert. Your task is to analyze the sentiment expressed in the given financial text.Only reply with positive, neutral, or negative.\",\n",
            "      \"role\": \"system\"\n",
            "    },\n",
            "    {\n",
            "      \"content\": \"More than 14,000 customers were left powerless .\",\n",
            "      \"role\": \"user\"\n",
            "    },\n",
            "    {\n",
            "      \"content\": \"negative\",\n",
            "      \"role\": \"assistant\"\n",
            "    }\n",
            "  ]\n",
            "}\n"
          ]
        }
      ],
      "source": [
        "# Preview a sample\n",
        "print(\"Sample training example:\")\n",
        "print(json.dumps(train_dataset[0], indent=2))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d8c1a8b8ba3043f78a809bb5e69eae06",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Creating json from Arrow format:   0%|          | 0/6 [00:00<?, ?ba/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "55470836b20547038c6b14952012b002",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Creating json from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saved training data to: /Users/ben/Code/ee/fine-tuning-spikes/sagemaker_projects/gemma-3-270m-it-finance-instruct/tmp_cache_local_dataset/train_data.jsonl\n",
            "Saved test data to: /Users/ben/Code/ee/fine-tuning-spikes/sagemaker_projects/gemma-3-270m-it-finance-instruct/tmp_cache_local_dataset/test_data.jsonl\n"
          ]
        }
      ],
      "source": [
        "# Save datasets to JSONL files\n",
        "train_filename = os.path.join(dataset_parent_path, \"train_data.jsonl\")\n",
        "test_filename = os.path.join(dataset_parent_path, \"test_data.jsonl\")\n",
        "\n",
        "train_dataset.to_json(train_filename, lines=True)\n",
        "test_dataset.to_json(test_filename, lines=True)\n",
        "\n",
        "print(f\"Saved training data to: {train_filename}\")\n",
        "print(f\"Saved test data to: {test_filename}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Upload Training Data to S3\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sagemaker.s3 import S3Uploader\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Uploaded training data to: s3://sagemaker-eu-west-2-889772146711/gemma-sentiment-finetune/dataset/train_data.jsonl\n"
          ]
        }
      ],
      "source": [
        "data_s3_uri = f\"s3://{sess.default_bucket()}/gemma-sentiment-finetune/dataset\"\n",
        "\n",
        "# Check if data already exists at this S3 location\n",
        "s3_client = boto3.client('s3')\n",
        "bucket = sess.default_bucket()\n",
        "prefix = \"gemma-sentiment-finetune/dataset/\"\n",
        "\n",
        "response = s3_client.list_objects_v2(Bucket=bucket, Prefix=prefix, MaxKeys=1)\n",
        "if response.get('KeyCount', 0) > 0:\n",
        "    existing_files = [obj['Key'] for obj in response.get('Contents', [])]\n",
        "    raise FileExistsError(\n",
        "        f\"S3 path already contains data!\\n\"\n",
        "        f\"Location: s3://{bucket}/{prefix}\\n\"\n",
        "        f\"Found: {existing_files}\\n\\n\"\n",
        "        f\"To overwrite, manually delete the existing data first:\\n\"\n",
        "        f\"  aws s3 rm s3://{bucket}/{prefix} --recursive\"\n",
        "    )\n",
        "\n",
        "uploaded_s3_uri = S3Uploader.upload(\n",
        "    local_path=train_filename,\n",
        "    desired_s3_uri=data_s3_uri\n",
        ")\n",
        "print(f\"Uploaded training data to: {uploaded_s3_uri}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Configure and Launch Training Job\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sagemaker.modules.configs import (\n",
        "    CheckpointConfig,\n",
        "    Compute,\n",
        "    InputData,\n",
        "    OutputDataConfig,\n",
        "    SourceCode,\n",
        "    StoppingCondition,\n",
        ")\n",
        "from sagemaker.modules.train import ModelTrainer\n",
        "from getpass import getpass\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "job_name: google--gemma-3-270m-it\n"
          ]
        }
      ],
      "source": [
        "MODEL_ID = \"google/gemma-3-270m-it\"\n",
        "# Enter your HuggingFace token (required for gated models like Gemma)\n",
        "hf_token = getpass(\"Enter your HuggingFace token: \")\n",
        "# Metrics will be reported to tensorboard\n",
        "reports_to = \"tensorboard\"\n",
        "# Job name\n",
        "job_name = MODEL_ID.replace('/', '--').replace('.', '-')\n",
        "print(f\"job_name: {job_name}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Training environment variables\n",
        "training_env = {\n",
        "    \"HF_TOKEN\": hf_token,\n",
        "    \"FI_EFA_USE_DEVICE_RDMA\": \"1\",\n",
        "    \"NCCL_DEBUG\": \"INFO\",\n",
        "    \"NCCL_SOCKET_IFNAME\": \"eth0\",\n",
        "    \"FI_PROVIDER\": \"efa\",\n",
        "    \"NCCL_PROTO\": \"simple\",\n",
        "    \"NCCL_NET_GDR_LEVEL\": \"5\"\n",
        "}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overwriting sagemaker_code/requirements.txt\n"
          ]
        }
      ],
      "source": [
        "%%writefile sagemaker_code/requirements.txt\n",
        "transformers==4.55.0\n",
        "peft==0.17.0\n",
        "accelerate==1.10.0\n",
        "bitsandbytes==0.46.1\n",
        "datasets==4.0.0\n",
        "deepspeed==0.16.4\n",
        "evaluate==0.4.5\n",
        "hf-transfer==0.1.8\n",
        "hf_xet\n",
        "liger-kernel==0.6.1\n",
        "lm-eval[api]==0.4.9\n",
        "kernels>=0.9.0\n",
        "mlflow\n",
        "safetensors>=0.6.2\n",
        "sagemaker==2.251.1\n",
        "sagemaker-mlflow==0.1.0\n",
        "sentencepiece==0.2.0\n",
        "scikit-learn==1.7.1\n",
        "tokenizers>=0.21.4\n",
        "triton\n",
        "trl==0.21.0\n",
        "py7zr\n",
        "nvidia-ml-py\n",
        "wandb\n",
        "git+https://github.com/triton-lang/triton.git@main#subdirectory=python/triton_kernels\n",
        "vllm==0.10.1\n",
        "poetry\n",
        "yq\n",
        "psutil\n",
        "nvidia-ml-py\n",
        "pyrsmi\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Training Configuration\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training instance: ml.g5.xlarge x 1\n"
          ]
        }
      ],
      "source": [
        "# Training arguments for PEFT LoRA\n",
        "args = [\n",
        "    \"--config\",\n",
        "    \"hf_recipes/google/gemma-3-270m-it--vanilla-peft-lora.yaml\",\n",
        "]\n",
        "\n",
        "# Instance configuration\n",
        "training_instance_type = \"ml.g5.xlarge\"  # A10G 1GPU 24GB\n",
        "training_instance_count = 1\n",
        "\n",
        "print(f\"Training instance: {training_instance_type} x {training_instance_count}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using image: 763104351884.dkr.ecr.eu-west-2.amazonaws.com/pytorch-training:2.7.1-gpu-py312\n"
          ]
        }
      ],
      "source": [
        "# Get the PyTorch training image\n",
        "pytorch_image_uri = sagemaker.image_uris.retrieve(\n",
        "    framework=\"pytorch\",\n",
        "    region=sess.boto_session.region_name,\n",
        "    version=\"2.7.1\",\n",
        "    instance_type=training_instance_type,\n",
        "    image_scope=\"training\",\n",
        ")\n",
        "print(f\"Using image: {pytorch_image_uri}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">[01/08/26 21:14:48] </span><span style=\"color: #d7af00; text-decoration-color: #d7af00; font-weight: bold\">WARNING </span> SageMaker session not provided. Using default Session.            <a href=\"file:///Users/ben/Code/ee/fine-tuning-spikes/.venv/lib/python3.12/site-packages/sagemaker/modules/train/model_trainer.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">model_trainer.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///Users/ben/Code/ee/fine-tuning-spikes/.venv/lib/python3.12/site-packages/sagemaker/modules/train/model_trainer.py#501\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">501</span></a>\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[2;36m[01/08/26 21:14:48]\u001b[0m\u001b[2;36m \u001b[0m\u001b[1;38;2;215;175;0mWARNING \u001b[0m SageMaker session not provided. Using default Session.            \u001b]8;id=167558;file:///Users/ben/Code/ee/fine-tuning-spikes/.venv/lib/python3.12/site-packages/sagemaker/modules/train/model_trainer.py\u001b\\\u001b[2mmodel_trainer.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=960214;file:///Users/ben/Code/ee/fine-tuning-spikes/.venv/lib/python3.12/site-packages/sagemaker/modules/train/model_trainer.py#501\u001b\\\u001b[2m501\u001b[0m\u001b]8;;\u001b\\\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span><span style=\"color: #d7af00; text-decoration-color: #d7af00; font-weight: bold\">WARNING </span> OutputDataConfig compression type not provided. Using default:    <a href=\"file:///Users/ben/Code/ee/fine-tuning-spikes/.venv/lib/python3.12/site-packages/sagemaker/modules/train/model_trainer.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">model_trainer.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///Users/ben/Code/ee/fine-tuning-spikes/.venv/lib/python3.12/site-packages/sagemaker/modules/train/model_trainer.py#582\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">582</span></a>\n",
              "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         GZIP                                                              <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">                    </span>\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[2;36m                   \u001b[0m\u001b[2;36m \u001b[0m\u001b[1;38;2;215;175;0mWARNING \u001b[0m OutputDataConfig compression type not provided. Using default:    \u001b]8;id=144071;file:///Users/ben/Code/ee/fine-tuning-spikes/.venv/lib/python3.12/site-packages/sagemaker/modules/train/model_trainer.py\u001b\\\u001b[2mmodel_trainer.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=10094;file:///Users/ben/Code/ee/fine-tuning-spikes/.venv/lib/python3.12/site-packages/sagemaker/modules/train/model_trainer.py#582\u001b\\\u001b[2m582\u001b[0m\u001b]8;;\u001b\\\n",
              "\u001b[2;36m                    \u001b[0m         GZIP                                                              \u001b[2m                    \u001b[0m\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span><span style=\"color: #0069ff; text-decoration-color: #0069ff; font-weight: bold\">INFO    </span> Training image URI:                                               <a href=\"file:///Users/ben/Code/ee/fine-tuning-spikes/.venv/lib/python3.12/site-packages/sagemaker/modules/train/model_trainer.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">model_trainer.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///Users/ben/Code/ee/fine-tuning-spikes/.venv/lib/python3.12/site-packages/sagemaker/modules/train/model_trainer.py#588\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">588</span></a>\n",
              "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">763104351884.</span>dkr.ecr.eu-west-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2.</span>amazonaws.com/pytorch-training:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2.7</span> <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">                    </span>\n",
              "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         .<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>-gpu-py312                                                      <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">                    </span>\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[2;36m                   \u001b[0m\u001b[2;36m \u001b[0m\u001b[1;38;2;0;105;255mINFO    \u001b[0m Training image URI:                                               \u001b]8;id=400050;file:///Users/ben/Code/ee/fine-tuning-spikes/.venv/lib/python3.12/site-packages/sagemaker/modules/train/model_trainer.py\u001b\\\u001b[2mmodel_trainer.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=375123;file:///Users/ben/Code/ee/fine-tuning-spikes/.venv/lib/python3.12/site-packages/sagemaker/modules/train/model_trainer.py#588\u001b\\\u001b[2m588\u001b[0m\u001b]8;;\u001b\\\n",
              "\u001b[2;36m                    \u001b[0m         \u001b[1;36m763104351884.\u001b[0mdkr.ecr.eu-west-\u001b[1;36m2.\u001b[0mamazonaws.com/pytorch-training:\u001b[1;36m2.7\u001b[0m \u001b[2m                    \u001b[0m\n",
              "\u001b[2;36m                    \u001b[0m         .\u001b[1;36m1\u001b[0m-gpu-py312                                                      \u001b[2m                    \u001b[0m\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "base_job_name: google--gemma-3-270m-it-sentiment-finetune\n",
            "output_path: s3://sagemaker-eu-west-2-889772146711/google--gemma-3-270m-it-sentiment-finetune\n"
          ]
        }
      ],
      "source": [
        "# Configure the ModelTrainer\n",
        "source_code = SourceCode(\n",
        "    source_dir=\"./sagemaker_code\",\n",
        "    command=f\"bash sm_accelerate_train.sh {' '.join(args)}\",\n",
        ")\n",
        "\n",
        "compute_configs = Compute(\n",
        "    instance_type=training_instance_type,\n",
        "    instance_count=training_instance_count,\n",
        "    keep_alive_period_in_seconds=1800,\n",
        "    volume_size_in_gb=125\n",
        ")\n",
        "\n",
        "base_job_name = f\"{job_name}-sentiment-finetune\"\n",
        "output_path = f\"s3://{sess.default_bucket()}/{base_job_name}\"\n",
        "\n",
        "model_trainer = ModelTrainer(\n",
        "    training_image=pytorch_image_uri,\n",
        "    source_code=source_code,\n",
        "    base_job_name=base_job_name,\n",
        "    compute=compute_configs,\n",
        "    stopping_condition=StoppingCondition(max_runtime_in_seconds=18000),\n",
        "    output_data_config=OutputDataConfig(\n",
        "        s3_output_path=output_path,\n",
        "    ),\n",
        "    checkpoint_config=CheckpointConfig(\n",
        "        s3_uri=os.path.join(\n",
        "            output_path,\n",
        "            \"sentiment-analysis\",\n",
        "            job_name,\n",
        "            \"checkpoints\"\n",
        "        ), \n",
        "        local_path=\"/opt/ml/checkpoints\"\n",
        "    ),\n",
        "    role=role,\n",
        "    environment=training_env\n",
        ")\n",
        "\n",
        "print(f\"base_job_name: {base_job_name}\")\n",
        "print(f\"output_path: {output_path}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Launch Training Job\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">[01/08/26 21:14:56] </span><span style=\"color: #d7af00; text-decoration-color: #d7af00; font-weight: bold\">WARNING </span> key_prefix is only applicable when data_source is a local file    <a href=\"file:///Users/ben/Code/ee/fine-tuning-spikes/.venv/lib/python3.12/site-packages/sagemaker/modules/train/model_trainer.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">model_trainer.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///Users/ben/Code/ee/fine-tuning-spikes/.venv/lib/python3.12/site-packages/sagemaker/modules/train/model_trainer.py#896\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">896</span></a>\n",
              "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         path.                                                             <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">                    </span>\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[2;36m[01/08/26 21:14:56]\u001b[0m\u001b[2;36m \u001b[0m\u001b[1;38;2;215;175;0mWARNING \u001b[0m key_prefix is only applicable when data_source is a local file    \u001b]8;id=929739;file:///Users/ben/Code/ee/fine-tuning-spikes/.venv/lib/python3.12/site-packages/sagemaker/modules/train/model_trainer.py\u001b\\\u001b[2mmodel_trainer.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=257086;file:///Users/ben/Code/ee/fine-tuning-spikes/.venv/lib/python3.12/site-packages/sagemaker/modules/train/model_trainer.py#896\u001b\\\u001b[2m896\u001b[0m\u001b]8;;\u001b\\\n",
              "\u001b[2;36m                    \u001b[0m         path.                                                             \u001b[2m                    \u001b[0m\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">[01/08/26 21:14:58] </span><span style=\"color: #0069ff; text-decoration-color: #0069ff; font-weight: bold\">INFO    </span> Creating training_job resource.                                     <a href=\"file:///Users/ben/Code/ee/fine-tuning-spikes/.venv/lib/python3.12/site-packages/sagemaker_core/main/resources.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">resources.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///Users/ben/Code/ee/fine-tuning-spikes/.venv/lib/python3.12/site-packages/sagemaker_core/main/resources.py#29245\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">29245</span></a>\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[2;36m[01/08/26 21:14:58]\u001b[0m\u001b[2;36m \u001b[0m\u001b[1;38;2;0;105;255mINFO    \u001b[0m Creating training_job resource.                                     \u001b]8;id=700424;file:///Users/ben/Code/ee/fine-tuning-spikes/.venv/lib/python3.12/site-packages/sagemaker_core/main/resources.py\u001b\\\u001b[2mresources.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=375411;file:///Users/ben/Code/ee/fine-tuning-spikes/.venv/lib/python3.12/site-packages/sagemaker_core/main/resources.py#29245\u001b\\\u001b[2m29245\u001b[0m\u001b]8;;\u001b\\\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">[01/08/26 21:14:59] </span><span style=\"color: #d7af00; text-decoration-color: #d7af00; font-weight: bold\">WARNING </span> Not displaing the training container logs as <span style=\"color: #008700; text-decoration-color: #008700\">'wait'</span> is set to     <a href=\"file:///Users/ben/Code/ee/fine-tuning-spikes/.venv/lib/python3.12/site-packages/sagemaker/modules/train/model_trainer.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">model_trainer.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///Users/ben/Code/ee/fine-tuning-spikes/.venv/lib/python3.12/site-packages/sagemaker/modules/train/model_trainer.py#834\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">834</span></a>\n",
              "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         <span style=\"color: #d70000; text-decoration-color: #d70000; font-style: italic\">False</span>.                                                            <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">                    </span>\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[2;36m[01/08/26 21:14:59]\u001b[0m\u001b[2;36m \u001b[0m\u001b[1;38;2;215;175;0mWARNING \u001b[0m Not displaing the training container logs as \u001b[38;2;0;135;0m'wait'\u001b[0m is set to     \u001b]8;id=800104;file:///Users/ben/Code/ee/fine-tuning-spikes/.venv/lib/python3.12/site-packages/sagemaker/modules/train/model_trainer.py\u001b\\\u001b[2mmodel_trainer.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=666033;file:///Users/ben/Code/ee/fine-tuning-spikes/.venv/lib/python3.12/site-packages/sagemaker/modules/train/model_trainer.py#834\u001b\\\u001b[2m834\u001b[0m\u001b]8;;\u001b\\\n",
              "\u001b[2;36m                    \u001b[0m         \u001b[3;38;2;215;0;0mFalse\u001b[0m.                                                            \u001b[2m                    \u001b[0m\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Training job launched!\n"
          ]
        }
      ],
      "source": [
        "# Launch the training job\n",
        "model_trainer.train(\n",
        "    input_data_config=[\n",
        "        InputData(\n",
        "            channel_name=\"training\",\n",
        "            data_source=uploaded_s3_uri,  \n",
        "        )\n",
        "    ], \n",
        "    wait=False  # Set to True to wait for completion\n",
        ")\n",
        "\n",
        "print(\"\\nTraining job launched!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "55b2a7a0e5ae465997c1afd294f49435",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Output()"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training job name: google--gemma-3-270m-it-sentiment-finetune-20260108211456\n",
            "Waiting for training to complete...\n",
            "\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">[01/09/26 05:43:15] </span><span style=\"color: #0069ff; text-decoration-color: #0069ff; font-weight: bold\">INFO    </span> Final Resource Status: <span style=\"font-weight: bold\">Completed</span>                                    <a href=\"file:///Users/ben/Code/ee/fine-tuning-spikes/.venv/lib/python3.12/site-packages/sagemaker_core/main/resources.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">resources.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///Users/ben/Code/ee/fine-tuning-spikes/.venv/lib/python3.12/site-packages/sagemaker_core/main/resources.py#29570\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">29570</span></a>\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[2;36m[01/09/26 05:43:15]\u001b[0m\u001b[2;36m \u001b[0m\u001b[1;38;2;0;105;255mINFO    \u001b[0m Final Resource Status: \u001b[1mCompleted\u001b[0m                                    \u001b]8;id=152504;file:///Users/ben/Code/ee/fine-tuning-spikes/.venv/lib/python3.12/site-packages/sagemaker_core/main/resources.py\u001b\\\u001b[2mresources.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=292327;file:///Users/ben/Code/ee/fine-tuning-spikes/.venv/lib/python3.12/site-packages/sagemaker_core/main/resources.py#29570\u001b\\\u001b[2m29570\u001b[0m\u001b]8;;\u001b\\\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
            ],
            "text/plain": []
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Wait for training to complete and stream logs\n",
        "training_job = model_trainer._latest_training_job\n",
        "print(f\"Training job name: {training_job.training_job_name}\")\n",
        "print(\"Waiting for training to complete...\\n\")\n",
        "training_job.wait(logs=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Next Steps\n",
        "\n",
        "Once the training job completes:\n",
        "\n",
        "1. The fine-tuned model will be saved to `{output_path}/model/`\n",
        "2. Run notebook 3 to deploy the model and run evaluations against the test set (`test_data.jsonl`)\n",
        "\n",
        "**Test data location:** `tmp_cache_local_dataset/test_data.jsonl` (200 samples held out for evaluation)\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
