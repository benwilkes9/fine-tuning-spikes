{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Evaluation: Gemma-3-270m-it Financial Sentiment Analysis\n",
        "\n",
        "This notebook evaluates the fine-tuned model against the base model on the test set.\n",
        "\n",
        "## Overview\n",
        "- **Model**: gemma-3-270m-it-bf16 (~270M parameters)\n",
        "- **Task**: Classify financial text as positive, negative, neutral, or bullish\n",
        "- **Evaluation**: Compare base model vs fine-tuned model accuracy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Setup and Configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import json\n",
        "from pathlib import Path\n",
        "from mlx_lm import load, generate\n",
        "import mlx.core as mx\n",
        "\n",
        "print(\"Setup complete!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Configuration\n",
        "MODEL_NAME = \"mlx-community/gemma-3-270m-it-bf16\"\n",
        "DATA_DIR = Path(\"data\")\n",
        "ADAPTER_PATH = Path(\"adapters\")\n",
        "\n",
        "print(f\"Model: {MODEL_NAME}\")\n",
        "print(f\"Data directory: {DATA_DIR}\")\n",
        "print(f\"Adapter path: {ADAPTER_PATH}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Evaluation\n",
        "\n",
        "Compare base model vs fine-tuned model on the test set using exact label matching."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def evaluate_model(model_path, adapter_path=None, test_file=None, max_samples=50):\n",
        "    \"\"\"\n",
        "    Evaluate model accuracy on sentiment classification.\n",
        "    \n",
        "    Args:\n",
        "        model_path: Path to base model\n",
        "        adapter_path: Path to LoRA adapters (optional)\n",
        "        test_file: Path to test data JSONL\n",
        "        max_samples: Number of samples to evaluate\n",
        "    \n",
        "    Returns:\n",
        "        dict with accuracy and predictions\n",
        "    \"\"\"\n",
        "    if test_file is None:\n",
        "        test_file = DATA_DIR / \"test.jsonl\"\n",
        "    \n",
        "    print(f\"Loading model from {model_path}\")\n",
        "    if adapter_path:\n",
        "        print(f\"Loading adapters from {adapter_path}\")\n",
        "    \n",
        "    model, tokenizer = load(model_path, adapter_path=adapter_path)\n",
        "    \n",
        "    valid_labels = {'positive', 'negative', 'neutral', 'bullish'}\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    predictions = []\n",
        "    \n",
        "    with open(test_file, 'r') as f:\n",
        "        for i, line in enumerate(f):\n",
        "            if i >= max_samples:\n",
        "                break\n",
        "            \n",
        "            data = json.loads(line)\n",
        "            messages = data['messages']\n",
        "            \n",
        "            # Expected label from assistant message\n",
        "            expected_label = messages[2]['content'].strip().lower()\n",
        "            \n",
        "            # Build prompt using chat template\n",
        "            prompt = tokenizer.apply_chat_template(\n",
        "                messages[:2],  # system + user only\n",
        "                add_generation_prompt=True,\n",
        "                tokenize=False\n",
        "            )\n",
        "            \n",
        "            # Generate response\n",
        "            response = generate(\n",
        "                model,\n",
        "                tokenizer,\n",
        "                prompt=prompt,\n",
        "                max_tokens=500,\n",
        "                verbose=False\n",
        "            )\n",
        "            \n",
        "            # Extract predicted label (first word that matches a valid label)\n",
        "            response_lower = response.lower()\n",
        "            predicted_label = None\n",
        "            for label in valid_labels:\n",
        "                if label in response_lower:\n",
        "                    predicted_label = label\n",
        "                    break\n",
        "            \n",
        "            is_correct = predicted_label == expected_label\n",
        "            if is_correct:\n",
        "                correct += 1\n",
        "            total += 1\n",
        "            \n",
        "            predictions.append({\n",
        "                'expected': expected_label,\n",
        "                'predicted': predicted_label,\n",
        "                'response': response,\n",
        "                'correct': is_correct\n",
        "            })\n",
        "            \n",
        "            if (i + 1) % 10 == 0:\n",
        "                print(f\"Evaluated {i + 1}/{max_samples} samples...\")\n",
        "    \n",
        "    accuracy = correct / total if total > 0 else 0\n",
        "    \n",
        "    return {\n",
        "        'accuracy': accuracy,\n",
        "        'correct': correct,\n",
        "        'total': total,\n",
        "        'predictions': predictions\n",
        "    }\n",
        "\n",
        "print(\"Evaluation function defined.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Evaluate base model (without fine-tuning)\n",
        "print(\"=\" * 60)\n",
        "print(\"EVALUATING BASE MODEL (no fine-tuning)\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "base_results = evaluate_model(\n",
        "    model_path=MODEL_NAME,\n",
        "    adapter_path=None,\n",
        "    max_samples=200  # Use more samples for reliable metrics (600 available)\n",
        ")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"BASE MODEL RESULTS\")\n",
        "print(\"=\" * 60)\n",
        "print(f\"Accuracy: {base_results['accuracy']:.2%}\")\n",
        "print(f\"Correct:  {base_results['correct']}/{base_results['total']}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Evaluate fine-tuned model\n",
        "print(\"=\" * 60)\n",
        "print(\"EVALUATING FINE-TUNED MODEL (with LoRA adapters)\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "finetuned_results = evaluate_model(\n",
        "    model_path=MODEL_NAME,\n",
        "    adapter_path=str(ADAPTER_PATH),\n",
        "    max_samples=200  # Use more samples for reliable metrics (600 available)\n",
        ")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"FINE-TUNED MODEL RESULTS\")\n",
        "print(\"=\" * 60)\n",
        "print(f\"Accuracy: {finetuned_results['accuracy']:.2%}\")\n",
        "print(f\"Correct:  {finetuned_results['correct']}/{finetuned_results['total']}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Compare results\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"COMPARISON\")\n",
        "print(\"=\" * 60)\n",
        "print(f\"Base Model Accuracy:       {base_results['accuracy']:.2%}\")\n",
        "print(f\"Fine-Tuned Model Accuracy: {finetuned_results['accuracy']:.2%}\")\n",
        "\n",
        "improvement = finetuned_results['accuracy'] - base_results['accuracy']\n",
        "print(f\"Improvement:               {improvement:+.2%}\")\n",
        "\n",
        "if improvement > 0:\n",
        "    if base_results['accuracy'] > 0:\n",
        "        factor = finetuned_results['accuracy'] / base_results['accuracy']\n",
        "        print(f\"\\nFine-tuning improved accuracy by {factor:.2f}x\")\n",
        "    else:\n",
        "        print(f\"\\nFine-tuning improved accuracy from 0% to {finetuned_results['accuracy']:.2%}\")\n",
        "elif improvement == 0:\n",
        "    print(\"\\nNo change in accuracy\")\n",
        "else:\n",
        "    print(\"\\nFine-tuning decreased accuracy (may indicate overfitting)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Show all prediction results as a table\n",
        "results_df = pd.DataFrame(finetuned_results['predictions'])\n",
        "results_df.index.name = '#'\n",
        "results_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Classification metrics for fine-tuned model\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Extract predictions, handling None values\n",
        "y_true = [p['expected'] for p in finetuned_results['predictions']]\n",
        "y_pred = [p['predicted'] or 'unknown' for p in finetuned_results['predictions']]\n",
        "\n",
        "labels = ['positive', 'negative', 'neutral', 'bullish']\n",
        "\n",
        "# Add 'unknown' to labels if any predictions failed to match\n",
        "if 'unknown' in y_pred:\n",
        "    display_labels = labels + ['unknown']\n",
        "else:\n",
        "    display_labels = labels\n",
        "\n",
        "# Confusion Matrix\n",
        "cm = confusion_matrix(y_true, y_pred, labels=display_labels)\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(8, 6))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
        "            xticklabels=display_labels, yticklabels=display_labels, ax=ax)\n",
        "ax.set_xlabel('Predicted')\n",
        "ax.set_ylabel('Actual')\n",
        "ax.set_title('Fine-tuned Model - Confusion Matrix')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Classification Report\n",
        "print(\"\\nClassification Report (Fine-tuned Model)\")\n",
        "print(\"=\" * 60)\n",
        "print(classification_report(y_true, y_pred, labels=labels, zero_division=0))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Base model prediction results\n",
        "base_df = pd.DataFrame(base_results['predictions'])\n",
        "base_df.index.name = '#'\n",
        "base_df"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
