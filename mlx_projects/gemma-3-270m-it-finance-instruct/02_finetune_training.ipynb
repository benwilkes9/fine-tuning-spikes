{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Fine-Tuning Gemma-3-270m-it for Financial Sentiment Analysis\n",
        "\n",
        "This notebook fine-tunes the `mlx-community/gemma-3-270m-it-bf16` model on a financial sentiment analysis task using MLX-LM with LoRA.\n",
        "\n",
        "## Overview\n",
        "- **Model**: gemma-3-270m-it-bf16 (~270M parameters)\n",
        "- **Task**: Classify financial text as positive, negative, neutral, or bullish\n",
        "- **Method**: LoRA (Low-Rank Adaptation)\n",
        "- **Data Split**: 80% train / 10% validation / 10% test\n",
        "\n",
        "## Hardware\n",
        "- MacBook Pro M4 Max\n",
        "- 40-core GPU\n",
        "- 64GB unified memory"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Setup and Configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import json\n",
        "import random\n",
        "from pathlib import Path\n",
        "from mlx_lm import load, generate\n",
        "import mlx.core as mx\n",
        "\n",
        "# Set random seed for reproducibility\n",
        "random.seed(42)\n",
        "mx.random.seed(42)\n",
        "\n",
        "print(\"Setup complete!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Configuration\n",
        "MODEL_NAME = \"mlx-community/gemma-3-270m-it-bf16\"\n",
        "DATA_DIR = Path(\"data\")\n",
        "ADAPTER_PATH = Path(\"adapters\")\n",
        "SOURCE_CSV = \"sentiment_training_data.csv\"\n",
        "\n",
        "# Create directories\n",
        "DATA_DIR.mkdir(exist_ok=True)\n",
        "ADAPTER_PATH.mkdir(exist_ok=True)\n",
        "\n",
        "print(f\"Model: {MODEL_NAME}\")\n",
        "print(f\"Data directory: {DATA_DIR}\")\n",
        "print(f\"Adapter path: {ADAPTER_PATH}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Data Preprocessing\n",
        "\n",
        "Load the sentiment dataset created in `01_data_analysis.ipynb` and convert to MLX-LM's conversational JSONL format with 80/10/10 split."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load source data\n",
        "df = pd.read_csv(SOURCE_CSV)\n",
        "\n",
        "print(f\"Loaded {len(df)} records\")\n",
        "print(f\"\\nColumns: {df.columns.tolist()}\")\n",
        "print(f\"\\nLabel distribution:\")\n",
        "print(df['assistant'].value_counts())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Preview sample data\n",
        "print(\"Sample entries:\")\n",
        "for i, row in df.sample(3, random_state=42).iterrows():\n",
        "    print(\"=\" * 60)\n",
        "    print(f\"SYSTEM: {row['system'][:100]}...\" if len(str(row['system'])) > 100 else f\"SYSTEM: {row['system']}\")\n",
        "    print(f\"USER: {row['user'][:200]}...\" if len(row['user']) > 200 else f\"USER: {row['user']}\")\n",
        "    print(f\"ASSISTANT: {row['assistant']}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Convert to conversational format\n",
        "data = []\n",
        "for _, row in df.iterrows():\n",
        "    # Use system prompt if available, otherwise use a default\n",
        "    system_content = row['system'].strip() if pd.notna(row['system']) and row['system'].strip() else \"You are a financial sentiment analysis expert. Analyze the sentiment of the given financial text and classify it as positive, negative, neutral, or bullish.\"\n",
        "    \n",
        "    conversation = {\n",
        "        \"messages\": [\n",
        "            {\n",
        "                \"role\": \"system\",\n",
        "                \"content\": system_content\n",
        "            },\n",
        "            {\n",
        "                \"role\": \"user\",\n",
        "                \"content\": row['user']\n",
        "            },\n",
        "            {\n",
        "                \"role\": \"assistant\",\n",
        "                \"content\": row['assistant']\n",
        "            }\n",
        "        ]\n",
        "    }\n",
        "    data.append(conversation)\n",
        "\n",
        "# Shuffle data\n",
        "random.shuffle(data)\n",
        "\n",
        "print(f\"Converted {len(data)} records to conversational format\")\n",
        "print(\"\\nExample:\")\n",
        "print(json.dumps(data[0], indent=2))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Split data: 80% train, 10% validation, 10% test\n",
        "total = len(data)\n",
        "train_end = int(total * 0.8)\n",
        "valid_end = int(total * 0.9)\n",
        "\n",
        "train_data = data[:train_end]\n",
        "valid_data = data[train_end:valid_end]\n",
        "test_data = data[valid_end:]\n",
        "\n",
        "print(f\"Train: {len(train_data)} records ({len(train_data)/total:.0%})\")\n",
        "print(f\"Valid: {len(valid_data)} records ({len(valid_data)/total:.0%})\")\n",
        "print(f\"Test:  {len(test_data)} records ({len(test_data)/total:.0%})\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save splits to JSONL files\n",
        "splits = {\n",
        "    'train.jsonl': train_data,\n",
        "    'valid.jsonl': valid_data,\n",
        "    'test.jsonl': test_data\n",
        "}\n",
        "\n",
        "for filename, split_data in splits.items():\n",
        "    filepath = DATA_DIR / filename\n",
        "    with open(filepath, 'w') as f:\n",
        "        for entry in split_data:\n",
        "            f.write(json.dumps(entry) + '\\n')\n",
        "    print(f\"Saved {filepath} ({len(split_data)} records)\")\n",
        "\n",
        "print(\"\\nData preprocessing complete!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Fine-Tuning with LoRA\n",
        "\n",
        "### Hyperparameters:\n",
        "- **batch_size**: 8 — Number of samples processed per training iteration.\n",
        "- **learning_rate**: 5e-5 — Step size for gradient descent optimization.\n",
        "- **lr_schedule**: cosine_decay with 10% warmup — Learning rate warms up then decays following a cosine curve.\n",
        "- **iters**: 1800 — Total number of training iterations.\n",
        "- **num_layers**: 18 — Number of transformer layers to apply LoRA adapters to.\n",
        "- **lora_rank**: 16 — Rank of the low-rank decomposition matrices.\n",
        "- **lora_scale**: 2.0 — Scaling factor applied to LoRA outputs (alpha/rank).\n",
        "- **lora_dropout**: 0.1 — Dropout probability for regularization.\n",
        "- **val_batches**: 25 — Number of batches used for validation evaluation.\n",
        "- **save_every**: 100 — Checkpoint saving frequency in iterations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Training configuration is defined in lora_config.yaml\n",
        "# This allows LoRA parameters (rank, scale, dropout) to be configured properly\n",
        "\n",
        "CONFIG_PATH = Path(\"lora_config.yaml\")\n",
        "\n",
        "# Display the config\n",
        "print(\"Training configuration (from lora_config.yaml):\")\n",
        "print(\"-\" * 50)\n",
        "print(CONFIG_PATH.read_text())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Run LoRA fine-tuning\n",
        "import subprocess\n",
        "\n",
        "print(\"Starting LoRA fine-tuning...\\n\")\n",
        "\n",
        "# Use config file which includes lora_parameters (rank, scale, dropout)\n",
        "cmd = [\n",
        "    \"python\", \"-m\", \"mlx_lm\", \"lora\",\n",
        "    \"--config\", str(CONFIG_PATH),\n",
        "]\n",
        "\n",
        "print(f\"Command: {' '.join(cmd)}\\n\")\n",
        "\n",
        "result = subprocess.run(cmd, capture_output=False, text=True)\n",
        "\n",
        "if result.returncode == 0:\n",
        "    print(\"\\nFine-tuning completed successfully!\")\n",
        "else:\n",
        "    print(f\"\\nFine-tuning failed with return code {result.returncode}\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
