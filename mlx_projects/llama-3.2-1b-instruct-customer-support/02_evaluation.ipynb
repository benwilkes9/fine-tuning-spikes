{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Evaluation: Llama-3.2-1B Customer Support Triage\n",
        "\n",
        "This notebook evaluates the fine-tuned model against the base model on the test set.\n",
        "\n",
        "## Overview\n",
        "- **Model**: Llama-3.2-1B-Instruct (~1B parameters)\n",
        "- **Task**: Convert customer tickets → internal bug reports with severity, owner, and investigation steps\n",
        "- **Evaluation**: Qualitative comparison of base model vs fine-tuned model outputs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Setup and Configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import json\n",
        "from pathlib import Path\n",
        "from mlx_lm import load, generate\n",
        "\n",
        "print(\"Setup complete!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Configuration\n",
        "MODEL_NAME = \"mlx-community/Llama-3.2-1B-Instruct-bf16\"\n",
        "DATA_DIR = Path(\"data\")\n",
        "ADAPTER_PATH = Path(\"adapters\")\n",
        "\n",
        "print(f\"Model: {MODEL_NAME}\")\n",
        "print(f\"Data directory: {DATA_DIR}\")\n",
        "print(f\"Adapter path: {ADAPTER_PATH}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Load Test Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import random\n",
        "\n",
        "# Choose data source: \"test\" for held-out test set, \"train\" for training sample (overfitted demo)\n",
        "DATA_SOURCE = \"train\"  # Options: \"test\" or \"train\"\n",
        "TRAIN_SAMPLE_SIZE = 5\n",
        "\n",
        "random.seed(42)\n",
        "\n",
        "if DATA_SOURCE == \"train\":\n",
        "    # Sample from training data (useful for demonstrating overfitted model)\n",
        "    all_train = []\n",
        "    with open(DATA_DIR / \"train.jsonl\", \"r\") as f:\n",
        "        for line in f:\n",
        "            line = line.strip()\n",
        "            if line:\n",
        "                all_train.append(json.loads(line))\n",
        "    eval_data = random.sample(all_train, min(TRAIN_SAMPLE_SIZE, len(all_train)))\n",
        "    print(f\"Sampled {len(eval_data)} records from training data ({len(all_train)} total)\")\n",
        "else:\n",
        "    # Use held-out test data\n",
        "    eval_data = []\n",
        "    with open(DATA_DIR / \"test.jsonl\", \"r\") as f:\n",
        "        for line in f:\n",
        "            line = line.strip()\n",
        "            if line:\n",
        "                eval_data.append(json.loads(line))\n",
        "    print(f\"Loaded {len(eval_data)} test samples\")\n",
        "\n",
        "# Preview first sample\n",
        "print(\"\\nSample entry:\")\n",
        "print(\"=\" * 60)\n",
        "sample = eval_data[0]\n",
        "user_msg = sample[\"messages\"][0][\"content\"]\n",
        "print(f\"USER INPUT:\\n{user_msg[:300]}...\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Generate Responses\n",
        "\n",
        "Run inference on all test samples using both the base model and the fine-tuned model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def generate_responses(model, tokenizer, test_data, max_tokens=500):\n",
        "    \"\"\"Generate responses for all test samples.\"\"\"\n",
        "    responses = []\n",
        "    for i, sample in enumerate(test_data):\n",
        "        user_content = sample[\"messages\"][0][\"content\"]\n",
        "\n",
        "        # Build prompt using chat template\n",
        "        prompt = tokenizer.apply_chat_template(\n",
        "            [{\"role\": \"user\", \"content\": user_content}],\n",
        "            add_generation_prompt=True,\n",
        "            tokenize=False\n",
        "        )\n",
        "\n",
        "        # Generate response\n",
        "        response = generate(\n",
        "            model,\n",
        "            tokenizer,\n",
        "            prompt=prompt,\n",
        "            max_tokens=max_tokens,\n",
        "            verbose=False\n",
        "        )\n",
        "        responses.append(response)\n",
        "        print(f\"Generated {i + 1}/{len(test_data)}\")\n",
        "\n",
        "    return responses\n",
        "\n",
        "print(\"Generation function defined.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load and run base model\n",
        "print(\"=\" * 60)\n",
        "print(\"LOADING BASE MODEL\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "base_model, base_tokenizer = load(MODEL_NAME)\n",
        "\n",
        "print(\"\\nGenerating responses with base model...\")\n",
        "base_responses = generate_responses(base_model, base_tokenizer, eval_data)\n",
        "\n",
        "print(\"\\nBase model generation complete!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load and run fine-tuned model\n",
        "print(\"=\" * 60)\n",
        "print(\"LOADING FINE-TUNED MODEL\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "finetuned_model, finetuned_tokenizer = load(MODEL_NAME, adapter_path=str(ADAPTER_PATH))\n",
        "\n",
        "print(\"\\nGenerating responses with fine-tuned model...\")\n",
        "finetuned_responses = generate_responses(finetuned_model, finetuned_tokenizer, eval_data)\n",
        "\n",
        "print(\"\\nFine-tuned model generation complete!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Compare Outputs\n",
        "\n",
        "Display side-by-side comparison of base model vs fine-tuned model outputs for each test sample."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from IPython.display import display, HTML\n",
        "import html as html_module\n",
        "\n",
        "def styled_comparison(idx, total, user_content, expected, finetuned, base):\n",
        "    \"\"\"Generate styled HTML for a single comparison (Equal Experts brand).\"\"\"\n",
        "    user_esc = html_module.escape(user_content).replace(\"\\n\", \"<br>\")\n",
        "    expected_esc = html_module.escape(expected).replace(\"\\n\", \"<br>\")\n",
        "    finetuned_esc = html_module.escape(finetuned).replace(\"\\n\", \"<br>\")\n",
        "    base_esc = html_module.escape(base).replace(\"\\n\", \"<br>\")\n",
        "\n",
        "    # Equal Experts Brand Colors\n",
        "    # Primary: EE Blue #1795D4, Secondary: Tech Blue #22567C\n",
        "    # Accents: Transform Teal #269C9E, Equal Ember #F07C00\n",
        "    # Neutrals: Dark Data #212526, The Cloud #F5F5F5, Byte White #FFFFFF\n",
        "\n",
        "    return f\"\"\"\n",
        "    <link href=\"https://fonts.googleapis.com/css2?family=Lexend:wght@300;400;500&display=swap\" rel=\"stylesheet\">\n",
        "    <div style=\"font-family: 'Lexend', sans-serif; margin: 24px 0; overflow: hidden; box-shadow: 0 2px 8px rgba(33,37,38,0.1); border: 1px solid #E0E0E0;\">\n",
        "        <div style=\"background: #22567C; color: #FFFFFF; padding: 18px 24px; font-size: 20px; font-weight: 400;\">\n",
        "            Sample {idx} of {total}\n",
        "        </div>\n",
        "\n",
        "        <div style=\"background: #FFFFFF; border-left: 6px solid #1795D4; padding: 18px 22px;\">\n",
        "            <div style=\"font-weight: 500; color: #22567C; margin-bottom: 12px; font-size: 13px; text-transform: uppercase; letter-spacing: 1px;\">\n",
        "                Customer Ticket\n",
        "            </div>\n",
        "            <div style=\"font-family: 'Lexend', sans-serif; font-weight: 300; font-size: 13px; color: #212526; line-height: 1.6;\">{user_esc}</div>\n",
        "        </div>\n",
        "\n",
        "        <div style=\"background: #F5F5F5; border-left: 6px solid #269C9E; padding: 18px 22px;\">\n",
        "            <div style=\"font-weight: 500; color: #269C9E; margin-bottom: 12px; font-size: 13px; text-transform: uppercase; letter-spacing: 1px;\">\n",
        "                Fine-Tuned Model Response\n",
        "            </div>\n",
        "            <div style=\"font-family: 'Lexend', sans-serif; font-weight: 300; font-size: 13px; color: #212526; line-height: 1.6;\">{finetuned_esc}</div>\n",
        "        </div>\n",
        "\n",
        "        <div style=\"background: #FFFFFF; border-left: 6px solid #212526; padding: 18px 22px;\">\n",
        "            <div style=\"font-weight: 500; color: #212526; margin-bottom: 12px; font-size: 13px; text-transform: uppercase; letter-spacing: 1px;\">\n",
        "                Base Model Response\n",
        "            </div>\n",
        "            <div style=\"font-family: 'Lexend', sans-serif; font-weight: 300; font-size: 13px; color: #212526; line-height: 1.6;\">{base_esc}</div>\n",
        "        </div>\n",
        "\n",
        "        <details style=\"background: #F5F5F5; border-left: 6px solid #F07C00; padding: 18px 22px;\">\n",
        "            <summary style=\"font-weight: 500; color: #22567C; font-size: 13px; text-transform: uppercase; letter-spacing: 1px; cursor: pointer; border-radius: 4px; padding: 4px 0;\">\n",
        "                Expected Response (Ground Truth) — click to expand\n",
        "            </summary>\n",
        "            <div style=\"font-family: 'Lexend', sans-serif; font-weight: 300; font-size: 13px; margin-top: 14px; color: #212526; line-height: 1.6;\">{expected_esc}</div>\n",
        "        </details>\n",
        "    </div>\n",
        "    \"\"\"\n",
        "\n",
        "# Display styled comparisons\n",
        "for i, sample in enumerate(eval_data):\n",
        "    user_content = sample[\"messages\"][0][\"content\"]\n",
        "    expected_output = sample[\"messages\"][1][\"content\"]\n",
        "\n",
        "    display(HTML(styled_comparison(\n",
        "        i + 1,\n",
        "        len(eval_data),\n",
        "        user_content,\n",
        "        expected_output,\n",
        "        finetuned_responses[i],\n",
        "        base_responses[i]\n",
        "    )))"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
