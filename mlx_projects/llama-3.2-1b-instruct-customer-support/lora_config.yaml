# MLX-LM LoRA Fine-Tuning Configuration

model: mlx-community/Llama-3.2-1B-Instruct-bf16
data: data
train: true
fine_tune_type: lora
adapter_path: adapters

# Training hyperparameters
batch_size: 6
num_layers: 16
iters: 250
learning_rate: 2.0e-05
val_batches: 1
save_every: 250
steps_per_eval: 50

# Learning rate schedule with warmup
lr_schedule:
  name: cosine_decay
  warmup: 25
  arguments: [2.0e-05, 225, 0.0]

# LoRA parameters
lora_parameters:
  rank: 16
  scale: 2.0
  dropout: 0.1
