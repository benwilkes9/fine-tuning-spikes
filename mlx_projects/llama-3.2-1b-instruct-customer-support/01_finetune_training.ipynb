{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Fine-Tuning Llama-3.2-1B-Instruct for Customer Support Triage\n",
        "\n",
        "This notebook fine-tunes `mlx-community/Llama-3.2-1B-Instruct-bf16` to transform customer support tickets into structured internal bug reports.\n",
        "\n",
        "## Overview\n",
        "- **Model**: Llama-3.2-1B-Instruct (~1B parameters)\n",
        "- **Task**: Convert customer tickets → internal bug reports with severity, owner, and investigation steps\n",
        "- **Method**: LoRA (Low-Rank Adaptation)\n",
        "- **Data Split**: 10 train / 1 valid / 1 test per category (stratified)\n",
        "\n",
        "## Hardware\n",
        "- MacBook Pro M4 Max\n",
        "- 40-core GPU\n",
        "- 64GB unified memory"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Setup and Configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Setup complete!\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "import random\n",
        "from pathlib import Path\n",
        "from collections import defaultdict\n",
        "\n",
        "# Set random seed for reproducibility\n",
        "random.seed(42)\n",
        "\n",
        "print(\"Setup complete!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: mlx-community/Llama-3.2-1B-Instruct-bf16\n",
            "Data directory: data\n",
            "Adapter path: adapters\n"
          ]
        }
      ],
      "source": [
        "# Configuration\n",
        "MODEL_NAME = \"mlx-community/Llama-3.2-1B-Instruct-bf16\"\n",
        "DATA_DIR = Path(\"data\")\n",
        "ADAPTER_PATH = Path(\"adapters\")\n",
        "SOURCE_FILE = \"source_data.jsonl\"\n",
        "\n",
        "# Create directories\n",
        "DATA_DIR.mkdir(exist_ok=True)\n",
        "ADAPTER_PATH.mkdir(exist_ok=True)\n",
        "\n",
        "print(f\"Model: {MODEL_NAME}\")\n",
        "print(f\"Data directory: {DATA_DIR}\")\n",
        "print(f\"Adapter path: {ADAPTER_PATH}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Load and Preview Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loaded 72 records\n",
            "\n",
            "Category distribution:\n",
            "  auth_p1: 12\n",
            "  auth_p2: 12\n",
            "  auth_p3: 12\n",
            "  data_platform_p1: 12\n",
            "  data_platform_p2: 12\n",
            "  data_platform_p3: 12\n"
          ]
        }
      ],
      "source": [
        "# Load source data\n",
        "data = []\n",
        "with open(SOURCE_FILE, \"r\") as f:\n",
        "    for line in f:\n",
        "        line = line.strip()\n",
        "        if line:  # Skip empty lines\n",
        "            data.append(json.loads(line))\n",
        "\n",
        "print(f\"Loaded {len(data)} records\")\n",
        "\n",
        "# Show category distribution\n",
        "category_counts = defaultdict(int)\n",
        "for entry in data:\n",
        "    category_counts[entry[\"category\"]] += 1\n",
        "\n",
        "print(\"\\nCategory distribution:\")\n",
        "for category, count in sorted(category_counts.items()):\n",
        "    print(f\"  {category}: {count}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sample entries:\n",
            "============================================================\n",
            "Category: data_platform_p2\n",
            "USER: Ticket #8045\n",
            "Customer: jennifer.hartley@novanexus.com\n",
            "Plan: Enterprise\n",
            "Issue: The filtering on our analytics dashboard is extremely laggy. Every time I change a filter dropdown it takes 20-30 seconds ...\n",
            "ASSISTANT: ## Internal Bug Report\n",
            "\n",
            "Severity: P2 (service degraded)\n",
            "\n",
            "Owner: Data Platform Team\n",
            "\n",
            "Summary: Dashboard filter interactions experiencing 20-30s latency since Monday\n",
            "\n",
            "Probable Cause: Filter queries hitt...\n",
            "============================================================\n",
            "Category: data_platform_p1\n",
            "USER: Ticket #8156\n",
            "Customer: tom.wright@pegasus-logistics.com\n",
            "Plan: Personal\n",
            "Issue: I can't export any data at all. Every export just spins forever then times out. I have a shipment manifest due to my bigge...\n",
            "ASSISTANT: ## Internal Bug Report\n",
            "\n",
            "Severity: P1 (business-critical)\n",
            "\n",
            "Owner: Data Platform Team\n",
            "\n",
            "Summary: Data exports timing out, blocking time-critical client deliverable\n",
            "\n",
            "Probable Cause: Snowflake ANALYTICS_WH...\n"
          ]
        }
      ],
      "source": [
        "# Preview sample entries\n",
        "print(\"Sample entries:\")\n",
        "for entry in random.sample(data, 2):\n",
        "    print(\"=\" * 60)\n",
        "    print(f\"Category: {entry['category']}\")\n",
        "    for msg in entry[\"messages\"]:\n",
        "        content_preview = msg[\"content\"][:200] + \"...\" if len(msg[\"content\"]) > 200 else msg[\"content\"]\n",
        "        print(f\"{msg['role'].upper()}: {content_preview}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Stratified Data Split\n",
        "\n",
        "Split data by category: 10 train / 1 validation / 1 test per category.\n",
        "\n",
        "This ensures each split has balanced representation across:\n",
        "- Teams (Data Platform, Auth)\n",
        "- Severity levels (P1, P2, P3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "auth_p1: 10 train, 1 valid, 1 test\n",
            "auth_p2: 10 train, 1 valid, 1 test\n",
            "auth_p3: 10 train, 1 valid, 1 test\n",
            "data_platform_p1: 10 train, 1 valid, 1 test\n",
            "data_platform_p2: 10 train, 1 valid, 1 test\n",
            "data_platform_p3: 10 train, 1 valid, 1 test\n",
            "\n",
            "Total: 60 train, 6 valid, 6 test\n"
          ]
        }
      ],
      "source": [
        "# Group by category\n",
        "by_category = defaultdict(list)\n",
        "for entry in data:\n",
        "    by_category[entry[\"category\"]].append(entry)\n",
        "\n",
        "# Stratified split: 10 train, 1 valid, 1 test per category\n",
        "train_data, valid_data, test_data = [], [], []\n",
        "\n",
        "for category, items in sorted(by_category.items()):\n",
        "    random.shuffle(items)\n",
        "    train_data.extend(items[:10])\n",
        "    valid_data.append(items[10])\n",
        "    test_data.append(items[11])\n",
        "    print(f\"{category}: {len(items[:10])} train, 1 valid, 1 test\")\n",
        "\n",
        "print(f\"\\nTotal: {len(train_data)} train, {len(valid_data)} valid, {len(test_data)} test\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Data split summary by category:\n",
            "\n",
            "Category              Train  Valid   Test\n",
            "------------------------------------------\n",
            "auth_p1                  10      1      1\n",
            "auth_p2                  10      1      1\n",
            "auth_p3                  10      1      1\n",
            "data_platform_p1         10      1      1\n",
            "data_platform_p2         10      1      1\n",
            "data_platform_p3         10      1      1\n",
            "------------------------------------------\n",
            "Total                    60      6      6\n"
          ]
        }
      ],
      "source": [
        "# Summarize the train_data, valid_data, test_data, grouped by category\n",
        "def count_by_category(entries):\n",
        "    counts = defaultdict(int)\n",
        "    for entry in entries:\n",
        "        counts[entry[\"category\"]] += 1\n",
        "    return counts\n",
        "\n",
        "train_counts = count_by_category(train_data)\n",
        "valid_counts = count_by_category(valid_data)\n",
        "test_counts = count_by_category(test_data)\n",
        "\n",
        "print(\"Data split summary by category:\\n\")\n",
        "print(f\"{'Category':<20} {'Train':>6} {'Valid':>6} {'Test':>6}\")\n",
        "print(\"-\" * 42)\n",
        "for category in sorted(set(train_counts) | set(valid_counts) | set(test_counts)):\n",
        "    print(f\"{category:<20} {train_counts[category]:>6} {valid_counts[category]:>6} {test_counts[category]:>6}\")\n",
        "print(\"-\" * 42)\n",
        "print(f\"{'Total':<20} {len(train_data):>6} {len(valid_data):>6} {len(test_data):>6}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Formatted data for MLX-LM\n",
            "\n",
            "Example entry:\n",
            "{\n",
            "  \"messages\": [\n",
            "    {\n",
            "      \"role\": \"user\",\n",
            "      \"content\": \"Ticket #6643\\nCustomer: support@quickship.logistics\\nPlan: Enterprise\\nIssue: Warehouse team reporting authentication failures across all 5 distribution centres. Drivers can't get manifests. We have 200+ shipments stuck. Error: \\\"session store unavailable\\\". CRITICAL.\"\n",
            "    },\n",
            "    {\n",
            "      \"role\": \"assistant\",\n",
            "      \"content\": \"## Internal Bug Report\\n\\nSeverity: P1 (business-critical)\\n\\nOwner: Auth Team\\n\\nSummary: Multi-site authen...\n"
          ]
        }
      ],
      "source": [
        "# Format for MLX-LM: remove category field, keep only messages\n",
        "def format_for_mlx(entries):\n",
        "    return [{\"messages\": entry[\"messages\"]} for entry in entries]\n",
        "\n",
        "train_formatted = format_for_mlx(train_data)\n",
        "valid_formatted = format_for_mlx(valid_data)\n",
        "test_formatted = format_for_mlx(test_data)\n",
        "\n",
        "print(\"Formatted data for MLX-LM\")\n",
        "print(\"\\nExample entry:\")\n",
        "print(json.dumps(train_formatted[0], indent=2)[:500] + \"...\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saved data/train.jsonl (60 records)\n",
            "Saved data/valid.jsonl (6 records)\n",
            "Saved data/test.jsonl (6 records)\n",
            "\n",
            "Data preprocessing complete!\n"
          ]
        }
      ],
      "source": [
        "# Save splits to JSONL files\n",
        "splits = {\n",
        "    \"train.jsonl\": train_formatted,\n",
        "    \"valid.jsonl\": valid_formatted,\n",
        "    \"test.jsonl\": test_formatted,\n",
        "}\n",
        "\n",
        "for filename, split_data in splits.items():\n",
        "    filepath = DATA_DIR / filename\n",
        "    with open(filepath, \"w\") as f:\n",
        "        for entry in split_data:\n",
        "            f.write(json.dumps(entry) + \"\\n\")\n",
        "    print(f\"Saved {filepath} ({len(split_data)} records)\")\n",
        "\n",
        "print(\"\\nData preprocessing complete!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Fine-Tuning with LoRA\n",
        "\n",
        "### Hyperparameters:\n",
        "- **batch_size**: 6 — Number of samples processed per training iteration (matches validation set size).\n",
        "- **learning_rate**: 2e-5 — Step size for gradient descent optimization.\n",
        "- **lr_schedule**: cosine_decay with 10% warmup — Learning rate warms up then decays following a cosine curve.\n",
        "- **iters**: 250 — Total number of training iterations (~25 epochs over 60 samples).\n",
        "- **num_layers**: 16 — Number of transformer layers to apply LoRA adapters to.\n",
        "- **lora_rank**: 16 — Rank of the low-rank decomposition matrices.\n",
        "- **lora_scale**: 2.0 — Scaling factor applied to LoRA outputs (alpha/rank).\n",
        "- **lora_dropout**: 0.1 — Dropout probability for regularization.\n",
        "- **val_batches**: 1 — Number of batches used for validation evaluation.\n",
        "- **steps_per_eval**: 50 — Validation every 50 iterations.\n",
        "- **save_every**: 250 — Checkpoint saving frequency in iterations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training configuration (from lora_config.yaml):\n",
            "--------------------------------------------------\n",
            "# MLX-LM LoRA Fine-Tuning Configuration\n",
            "\n",
            "model: mlx-community/Llama-3.2-1B-Instruct-bf16\n",
            "data: data\n",
            "train: true\n",
            "fine_tune_type: lora\n",
            "adapter_path: adapters\n",
            "\n",
            "# Training hyperparameters\n",
            "batch_size: 6\n",
            "num_layers: 16\n",
            "iters: 250\n",
            "learning_rate: 2.0e-05\n",
            "val_batches: 1\n",
            "save_every: 250\n",
            "steps_per_eval: 50\n",
            "\n",
            "# Learning rate schedule with warmup\n",
            "lr_schedule:\n",
            "  name: cosine_decay\n",
            "  warmup: 25\n",
            "  arguments: [2.0e-05, 225, 0.0]\n",
            "\n",
            "# LoRA parameters\n",
            "lora_parameters:\n",
            "  rank: 16\n",
            "  scale: 2.0\n",
            "  dropout: 0.1\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from pathlib import Path\n",
        "\n",
        "# Training configuration is defined in lora_config.yaml\n",
        "CONFIG_PATH = Path(\"lora_config.yaml\")\n",
        "\n",
        "print(\"Training configuration (from lora_config.yaml):\")\n",
        "print(\"-\" * 50)\n",
        "print(CONFIG_PATH.read_text())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Starting LoRA fine-tuning...\n",
            "\n",
            "Command: python -m mlx_lm lora --config lora_config.yaml\n",
            "\n",
            "Loading configuration file lora_config.yaml\n",
            "Loading pretrained model\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Fetching 6 files: 100%|██████████| 6/6 [00:00<00:00, 16090.68it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading datasets\n",
            "Training\n",
            "Trainable parameters: 0.912% (11.272M/1235.814M)\n",
            "Starting training..., iters: 250\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Calculating loss...: 100%|██████████| 1/1 [00:00<00:00,  1.53it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Iter 1: Val loss 4.389, Val took 0.656s\n",
            "Iter 10: Train loss 4.273, Learning Rate 7.200e-06, It/sec 1.074, Tokens/sec 1392.033, Trained Tokens 12964, Peak mem 10.412 GB\n",
            "Iter 20: Train loss 3.222, Learning Rate 1.520e-05, It/sec 1.424, Tokens/sec 1846.481, Trained Tokens 25928, Peak mem 10.441 GB\n",
            "Iter 30: Train loss 2.272, Learning Rate 1.999e-05, It/sec 1.347, Tokens/sec 1745.815, Trained Tokens 38892, Peak mem 10.441 GB\n",
            "Iter 40: Train loss 1.832, Learning Rate 1.984e-05, It/sec 1.325, Tokens/sec 1717.778, Trained Tokens 51856, Peak mem 10.441 GB\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Calculating loss...: 100%|██████████| 1/1 [00:00<00:00,  2.71it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Iter 50: Val loss 1.707, Val took 0.372s\n",
            "Iter 50: Train loss 1.596, Learning Rate 1.949e-05, It/sec 1.317, Tokens/sec 1706.947, Trained Tokens 64820, Peak mem 10.441 GB\n",
            "Iter 60: Train loss 1.418, Learning Rate 1.896e-05, It/sec 1.303, Tokens/sec 1688.755, Trained Tokens 77784, Peak mem 10.441 GB\n",
            "Iter 70: Train loss 1.258, Learning Rate 1.825e-05, It/sec 1.297, Tokens/sec 1681.050, Trained Tokens 90748, Peak mem 10.441 GB\n",
            "Iter 80: Train loss 1.110, Learning Rate 1.738e-05, It/sec 1.284, Tokens/sec 1665.097, Trained Tokens 103712, Peak mem 10.441 GB\n",
            "Iter 90: Train loss 0.959, Learning Rate 1.637e-05, It/sec 1.313, Tokens/sec 1702.081, Trained Tokens 116676, Peak mem 10.441 GB\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Calculating loss...: 100%|██████████| 1/1 [00:00<00:00,  2.46it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Iter 100: Val loss 1.779, Val took 0.409s\n",
            "Iter 100: Train loss 0.811, Learning Rate 1.524e-05, It/sec 1.291, Tokens/sec 1674.018, Trained Tokens 129640, Peak mem 10.441 GB\n",
            "Iter 110: Train loss 0.675, Learning Rate 1.400e-05, It/sec 1.277, Tokens/sec 1655.673, Trained Tokens 142604, Peak mem 10.441 GB\n",
            "Iter 120: Train loss 0.557, Learning Rate 1.269e-05, It/sec 1.253, Tokens/sec 1624.440, Trained Tokens 155568, Peak mem 10.441 GB\n",
            "Iter 130: Train loss 0.442, Learning Rate 1.132e-05, It/sec 1.293, Tokens/sec 1675.905, Trained Tokens 168532, Peak mem 10.441 GB\n",
            "Iter 140: Train loss 0.356, Learning Rate 9.930e-06, It/sec 1.298, Tokens/sec 1682.644, Trained Tokens 181496, Peak mem 10.441 GB\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Calculating loss...: 100%|██████████| 1/1 [00:00<00:00,  2.61it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Iter 150: Val loss 2.251, Val took 0.385s\n",
            "Iter 150: Train loss 0.297, Learning Rate 8.539e-06, It/sec 1.291, Tokens/sec 1673.205, Trained Tokens 194460, Peak mem 10.441 GB\n",
            "Iter 160: Train loss 0.250, Learning Rate 7.177e-06, It/sec 1.223, Tokens/sec 1585.141, Trained Tokens 207424, Peak mem 10.441 GB\n",
            "Iter 170: Train loss 0.221, Learning Rate 5.869e-06, It/sec 1.278, Tokens/sec 1657.095, Trained Tokens 220388, Peak mem 10.441 GB\n",
            "Iter 180: Train loss 0.197, Learning Rate 4.642e-06, It/sec 1.281, Tokens/sec 1660.720, Trained Tokens 233352, Peak mem 10.441 GB\n",
            "Iter 190: Train loss 0.182, Learning Rate 3.519e-06, It/sec 1.250, Tokens/sec 1621.011, Trained Tokens 246316, Peak mem 10.441 GB\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Calculating loss...: 100%|██████████| 1/1 [00:00<00:00,  2.46it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Iter 200: Val loss 2.634, Val took 0.408s\n",
            "Iter 200: Train loss 0.173, Learning Rate 2.522e-06, It/sec 1.235, Tokens/sec 1601.067, Trained Tokens 259280, Peak mem 10.441 GB\n",
            "Iter 210: Train loss 0.168, Learning Rate 1.671e-06, It/sec 1.234, Tokens/sec 1599.375, Trained Tokens 272244, Peak mem 10.441 GB\n",
            "Iter 220: Train loss 0.163, Learning Rate 9.817e-07, It/sec 1.207, Tokens/sec 1564.180, Trained Tokens 285208, Peak mem 10.441 GB\n",
            "Iter 230: Train loss 0.162, Learning Rate 4.681e-07, It/sec 1.164, Tokens/sec 1509.533, Trained Tokens 298172, Peak mem 10.441 GB\n",
            "Iter 240: Train loss 0.161, Learning Rate 1.400e-07, It/sec 1.130, Tokens/sec 1465.276, Trained Tokens 311136, Peak mem 10.441 GB\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Calculating loss...: 100%|██████████| 1/1 [00:00<00:00,  2.23it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Iter 250: Val loss 2.695, Val took 0.451s\n",
            "Iter 250: Train loss 0.160, Learning Rate 3.899e-09, It/sec 1.105, Tokens/sec 1432.721, Trained Tokens 324100, Peak mem 10.441 GB\n",
            "Iter 250: Saved adapter weights to adapters/adapters.safetensors and adapters/0000250_adapters.safetensors.\n",
            "Saved final weights to adapters/adapters.safetensors.\n",
            "\n",
            "Fine-tuning completed successfully!\n"
          ]
        }
      ],
      "source": [
        "# Run LoRA fine-tuning\n",
        "import subprocess\n",
        "\n",
        "print(\"Starting LoRA fine-tuning...\\n\")\n",
        "\n",
        "cmd = [\n",
        "    \"python\", \"-m\", \"mlx_lm\", \"lora\",\n",
        "    \"--config\", str(CONFIG_PATH),\n",
        "]\n",
        "\n",
        "print(f\"Command: {' '.join(cmd)}\\n\")\n",
        "\n",
        "result = subprocess.run(cmd, capture_output=False, text=True)\n",
        "\n",
        "if result.returncode == 0:\n",
        "    print(\"\\nFine-tuning completed successfully!\")\n",
        "else:\n",
        "    print(f\"\\nFine-tuning failed with return code {result.returncode}\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
